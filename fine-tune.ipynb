{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c51f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from huggingface_hub import login\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor,\n",
    "    WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training, LoraConfig,\n",
    "    get_peft_model, PeftModel, PeftConfig\n",
    ")\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fac270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. ENVIRONMENT & SETUP\n",
    "# ------------------------------------------------------------\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"None\"\n",
    "login(token=\"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\", add_to_git_credential=True)\n",
    "\n",
    "model_name_or_path = \"openai/whisper-large-v3\"\n",
    "task = \"transcribe\"\n",
    "#dataset_path = \"zh_health_dataset\"\n",
    "#sentence_counts = [1288, 416, 1843, 33, 3739, 3602, 1583, 2227, 58, 116, 177, 2574, 187, 13, 919, 276, 637, 130, 679, 666,556, 69, 516]\n",
    "\n",
    "#data = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8212415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. SPLIT DATASET BY LANGUAGE COUNTS\n",
    "# ------------------------------------------------------------\n",
    "def split_multilang_dataset(dataset, counts):\n",
    "    start_index = 0\n",
    "    train, test, eval = [], [], []\n",
    "    for count in counts:\n",
    "        end_index = start_index + count\n",
    "        if count != 1:\n",
    "            train.append(dataset.select(range(start_index, start_index + int(count * 0.8))))\n",
    "            test.append(dataset.select(range(start_index + int(count * 0.8), start_index + int(count * 0.9))))\n",
    "            eval.append(dataset.select(range(start_index + int(count * 0.9), start_index + count)))\n",
    "        else:\n",
    "            train.append(dataset.select(range(start_index, start_index + 1)))\n",
    "        start_index = end_index\n",
    "    return concatenate_datasets(train), concatenate_datasets(test), concatenate_datasets(eval)\n",
    "\n",
    "#train_data, test_data, eval_data = split_multilang_dataset(data, sentence_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3bc14e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enulu/anaconda3/envs/voice/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. PROCESSOR & COLLATOR\n",
    "# ------------------------------------------------------------\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, task=task)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features):\n",
    "        input_feats = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_feats, return_tensors=\"pt\")\n",
    "        label_feats = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_feats, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dba66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(train_data, val_data, output_dir, lang_code):\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, device_map=\"auto\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.model.encoder.conv1.register_forward_hook(lambda m, i, o: o.requires_grad_(True))\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05, bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1e-6,\n",
    "        warmup_steps=50,\n",
    "        eval_steps=100,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=3,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        generation_max_length=128,\n",
    "        logging_steps=1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    del trainer, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_from_encoded(model_path, dataset):\n",
    "    from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "    from datasets import load_from_disk\n",
    "    import torch\n",
    "    import jiwer\n",
    "    from jiwer import wer, Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Normalizer\n",
    "    nwer_norm = Compose([ToLowerCase(), RemovePunctuation(), RemoveMultipleSpaces(), Strip()])\n",
    "\n",
    "    print(f\"ğŸ”¹ Veriset yÃ¼kleniyor: {dataset_path}\")\n",
    "    dataset = dataset\n",
    "\n",
    "    print(f\"ğŸ”¹ Model yÃ¼kleniyor: {model_path}\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_path, device_map=\"auto\", )\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    all_predictions, all_references = [], []\n",
    "\n",
    "    print(\"ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\")\n",
    "    for sample in tqdm(dataset):\n",
    "        input_feats = torch.tensor(sample[\"input_features\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        try:\n",
    "            label_ids = [token for token in sample[\"labels\"] if token != -100]\n",
    "            reference = processor.tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Referans decode hatasÄ±: {e}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_ids = model.generate(input_feats)\n",
    "        prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        all_predictions.append(prediction)\n",
    "        all_references.append(reference)\n",
    "\n",
    "    overall_wer = wer(all_references, all_predictions)\n",
    "    normalized_references = [nwer_norm(r) for r in all_references]\n",
    "    normalized_predictions = [nwer_norm(p) for p in all_predictions]\n",
    "    overall_nwer = wer(normalized_references, normalized_predictions)\n",
    "\n",
    "    print(f\"\\nâœ… WER: {overall_wer:.4f}\")\n",
    "    print(f\"âœ… Normalized WER: {overall_nwer:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"WER\": overall_wer * 100,\n",
    "        \"Normalized_WER\": overall_nwer * 100\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6623814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== â³ Evaluating: AR - LAW ===\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ar\n",
      "ğŸ”¹ Model yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/fine-tuned_modeller/law/ar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enulu/anaconda3/envs/voice/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/enulu/Workspace/Domain_based/fine-tuned_modeller/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# veya \"openai/whisper-large-v3\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_from_encoded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m deÄŸerlendirme baÅŸarÄ±sÄ±z: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mevaluate_from_encoded\u001b[0;34m(model_path, dataset)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”¹ Model yÃ¼kleniyor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m processor \u001b[38;5;241m=\u001b[39m WhisperProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-large-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     19\u001b[0m device \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/anaconda3/envs/voice/lib/python3.9/site-packages/transformers/modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3523\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3524\u001b[0m     (\n\u001b[1;32m   3525\u001b[0m         model,\n\u001b[1;32m   3526\u001b[0m         missing_keys,\n\u001b[1;32m   3527\u001b[0m         unexpected_keys,\n\u001b[1;32m   3528\u001b[0m         mismatched_keys,\n\u001b[1;32m   3529\u001b[0m         offload_index,\n\u001b[1;32m   3530\u001b[0m         error_msgs,\n\u001b[0;32m-> 3531\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3538\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3542\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3543\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3549\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/voice/lib/python3.9/site-packages/transformers/modeling_utils.py:3958\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3954\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   3955\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   3956\u001b[0m                 )\n\u001b[1;32m   3957\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3958\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3975\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/voice/lib/python3.9/site-packages/transformers/modeling_utils.py:778\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 778\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_param\u001b[38;5;241m.\u001b[39mis_contiguous():\n\u001b[1;32m    781\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "domains = [\"law\", \"health\"]\n",
    "languages = [\"ar\", \"cs\", \"de\", \"el\", \"en\", \"es\", \"fa\", \"fr\", \"he\", \"hi\", \"id\", \"it\",\n",
    "             \"ja\", \"ko\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "        print(f\"\\n=== â³ Evaluating: {lang.upper()} - {domain.upper()} ===\")\n",
    "\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}-{domain} verisi yÃ¼klenemedi: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Split dataset\n",
    "        total = len(dataset)\n",
    "        train_data = dataset.select(range(0, int(total * 0.8)))\n",
    "        eval_data = dataset.select(range(int(total * 0.8), int(total * 0.9)))\n",
    "        test_data = dataset.select(range(int(total * 0.9), total))\n",
    "\n",
    "\n",
    "\n",
    "        model_path = f\"/home/enulu/Workspace/Domain_based/fine-tuned_modeller/{domain}/{lang}\"  # veya \"openai/whisper-large-v3\"\n",
    "\n",
    "        try:\n",
    "            metrics = evaluate_from_encoded(model_path, test_data)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {lang}-{domain} deÄŸerlendirme baÅŸarÄ±sÄ±z: {e}\")\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"domain\": domain,\n",
    "            \"lang\": lang,\n",
    "            **metrics\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5d8a90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: AR - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ar\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:24<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.5282\n",
      "âœ… Normalized WER: 0.4488\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: CS - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/cs\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:24<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1149\n",
      "âœ… Normalized WER: 0.0743\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: DE - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/de\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [02:22<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0401\n",
      "âœ… Normalized WER: 0.0215\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: EL - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/el\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1333\n",
      "âœ… Normalized WER: 0.0000\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: EN - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/en\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 387/387 [03:06<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1041\n",
      "âœ… Normalized WER: 0.0614\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: ES - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/es\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 273/273 [02:27<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0741\n",
      "âœ… Normalized WER: 0.0288\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: FA - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/fa\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [00:54<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.3393\n",
      "âœ… Normalized WER: 0.3016\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: FR - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/fr\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 298/298 [02:42<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1349\n",
      "âœ… Normalized WER: 0.0761\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: HE - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/he\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.4062\n",
      "âœ… Normalized WER: 0.2812\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: HI - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/hi\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.6286\n",
      "âœ… Normalized WER: 0.6286\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: ID - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/id\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0900\n",
      "âœ… Normalized WER: 0.0700\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: IT - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/it\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [01:53<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0883\n",
      "âœ… Normalized WER: 0.0431\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: JA - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ja\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:06<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.8182\n",
      "âœ… Normalized WER: 0.2727\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: KO - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ko\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.3333\n",
      "âœ… Normalized WER: 0.0000\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: NL - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/nl\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [01:49<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0698\n",
      "âœ… Normalized WER: 0.0557\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: PL - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/pl\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:22<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1616\n",
      "âœ… Normalized WER: 0.0279\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: PT - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/pt\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:29<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.2500\n",
      "âœ… Normalized WER: 0.1148\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: RO - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ro\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:16<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0909\n",
      "âœ… Normalized WER: 0.0537\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: RU - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ru\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:50<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0560\n",
      "âœ… Normalized WER: 0.0274\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: TR - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/tr\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:11<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1333\n",
      "âœ… Normalized WER: 0.0963\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: UK - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/uk\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:20<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.2103\n",
      "âœ… Normalized WER: 0.1536\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: VI - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/vi\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1429\n",
      "âœ… Normalized WER: 0.0000\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: ZH - LAW\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/zh\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:16<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 1.0357\n",
      "âœ… Normalized WER: 0.7143\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: AR - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ar\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [01:00<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.4150\n",
      "âœ… Normalized WER: 0.2912\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: CS - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/cs\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:23<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1049\n",
      "âœ… Normalized WER: 0.0839\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: DE - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/de\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [01:36<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0561\n",
      "âœ… Normalized WER: 0.0376\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: EL - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/el\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1471\n",
      "âœ… Normalized WER: 0.0294\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: EN - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/en\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 374/374 [03:03<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1944\n",
      "âœ… Normalized WER: 0.1721\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: ES - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/es\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 361/361 [03:02<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0520\n",
      "âœ… Normalized WER: 0.0236\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: FA - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/fa\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159/159 [01:27<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.3266\n",
      "âœ… Normalized WER: 0.2911\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: FR - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/fr\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [01:55<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0996\n",
      "âœ… Normalized WER: 0.0608\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: HE - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/he\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.2174\n",
      "âœ… Normalized WER: 0.0870\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: HI - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/hi\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:10<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.2336\n",
      "âœ… Normalized WER: 0.2150\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: ID - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/id\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:10<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1062\n",
      "âœ… Normalized WER: 0.0750\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: IT - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/it\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [02:24<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0612\n",
      "âœ… Normalized WER: 0.0357\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: JA - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ja\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 1.0000\n",
      "âœ… Normalized WER: 0.6316\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: KO - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ko\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.2000\n",
      "âœ… Normalized WER: 0.2000\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: NL - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/nl\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:51<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0680\n",
      "âœ… Normalized WER: 0.0592\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: PL - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/pl\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:16<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1783\n",
      "âœ… Normalized WER: 0.0306\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: PT - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/pt\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:31<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1937\n",
      "âœ… Normalized WER: 0.0631\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: RO - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ro\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.2143\n",
      "âœ… Normalized WER: 0.1531\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: RU - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ru\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:39<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.0794\n",
      "âœ… Normalized WER: 0.0468\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: TR - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/tr\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:33<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1716\n",
      "âœ… Normalized WER: 0.1099\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: UK - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/uk\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.2129\n",
      "âœ… Normalized WER: 0.1639\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: VI - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/vi\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.1343\n",
      "âœ… Normalized WER: 0.0149\n",
      "\n",
      "ğŸŒ Evaluating ORIGINAL Whisper: ZH - HEALTH\n",
      "ğŸ”¹ Veriset yÃ¼kleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/zh\n",
      "ğŸ”¹ Model yÃ¼kleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DeÄŸerlendirme baÅŸlatÄ±ldÄ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:31<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… WER: 0.9245\n",
      "âœ… Normalized WER: 0.7170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "original_results = []\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "        print(f\"\\nğŸŒ Evaluating ORIGINAL Whisper: {lang.upper()} - {domain.upper()}\")\n",
    "\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        model_path = \"openai/whisper-large-v3\"  # Orijinal Hugging Face modeli\n",
    "\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "            dataset = dataset.shuffle(seed=42)\n",
    "            total = len(dataset)\n",
    "            test_data = dataset.select(range(int(0.9 * total), total))\n",
    "\n",
    "\n",
    "\n",
    "            metrics = evaluate_from_encoded(model_path, test_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[âŒ ERROR] {lang}-{domain} (original): {e}\")\n",
    "            continue\n",
    "\n",
    "        original_results.append({\n",
    "            \"domain\": domain,\n",
    "            \"lang\": lang,\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "# CSV olarak orijinal sonuÃ§larÄ± kaydet\n",
    "df_orig = pd.DataFrame(original_results)\n",
    "df_orig.to_csv(\"original_whisper_eval_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac89475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"eval_result_latest.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6. EVALUATION\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_model(model_id, dataset, processor, lang_code, task=\"transcribe\"):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned Whisper model with WER and normalized WER.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): Path to the fine-tuned model.\n",
    "        dataset (Dataset): Hugging Face dataset to evaluate.\n",
    "        processor: WhisperProcessor instance.\n",
    "        lang_code (str): ISO language code (e.g., \"tr\", \"en\", \"zh\").\n",
    "        task (str): \"transcribe\" or \"translate\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with WER and normalized WER.\n",
    "    \"\"\"\n",
    "    peft_config = PeftConfig.from_pretrained(model_id)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        peft_config.base_model_name_or_path, device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model, model_id)\n",
    "    model.eval()\n",
    "\n",
    "    normalizer = BasicTextNormalizer()\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    dataloader = DataLoader(dataset, batch_size=5, collate_fn=data_collator)\n",
    "\n",
    "    # Try assigning forced_decoder_ids based on language\n",
    "    try:\n",
    "        forced_decoder_ids = processor.get_decoder_prompt_ids(language=lang_code, task=task)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] {lang_code} iÃ§in forced_decoder_ids kullanÄ±lamadÄ±: {e}\")\n",
    "        forced_decoder_ids = None\n",
    "\n",
    "    preds, refs, norm_preds, norm_refs = [], [], [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Evaluating {lang_code}\"):\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            input_features = batch[\"input_features\"].to(\"cuda\")\n",
    "\n",
    "            generated = model.generate(\n",
    "                input_features=input_features,\n",
    "                forced_decoder_ids=forced_decoder_ids,\n",
    "                max_new_tokens=255\n",
    "            )\n",
    "\n",
    "            labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "            decoded_preds = processor.tokenizer.batch_decode(\n",
    "                generated.cpu().numpy(), skip_special_tokens=True)\n",
    "            decoded_labels = processor.tokenizer.batch_decode(\n",
    "                np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), processor.tokenizer.pad_token_id),\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            preds.extend(decoded_preds)\n",
    "            refs.extend(decoded_labels)\n",
    "            norm_preds.extend([normalizer(p).strip() for p in decoded_preds])\n",
    "            norm_refs.extend([normalizer(r).strip() for r in decoded_labels])\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=preds, references=refs)\n",
    "    norm_wer = 100 * metric.compute(predictions=norm_preds, references=norm_refs)\n",
    "\n",
    "    print(f\"[RESULT] {lang_code} WER: {wer:.2f}, Normalized WER: {norm_wer:.2f}\")\n",
    "    return {\"WER\": wer, \"Normalized_WER\": norm_wer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b77fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"domains = [\"law\", \"health\"]\n",
    "languages = [\"ar\", \"cs\", \"de\", \"el\", \"en\", \"es\", \"fa\", \"fr\", \"he\", \"hi\", \"id\", \"it\",\n",
    "             \"ja\", \"ko\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "        # Load dataset\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}-{domain} verisi yÃ¼klenemedi: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Split dataset\n",
    "        total = len(dataset)\n",
    "        train_data = dataset.select(range(0, int(total * 0.8)))\n",
    "        eval_data = dataset.select(range(int(total * 0.8), int(total * 0.9)))\n",
    "        test_data = dataset.select(range(int(total * 0.9), total))\n",
    "\n",
    "        # Fine-tune model (veya Ã¶nceden eÄŸitilmiÅŸ modeli yÃ¼kle)\n",
    "        model_path = f\"/home/enulu/Workspace/Domain_based/fine-tuned_modeller/fine-tuned-{domain}\"\n",
    "        if not os.path.exists(model_path):\n",
    "            fine_tune_model(train_data, eval_data, model_path, lang)\n",
    "\n",
    "        # Evaluate\n",
    "        metrics = evaluate_model(model_path, test_data, processor, lang)\n",
    "        print(f\"{lang}-{domain} WER:\", metrics)\n",
    "        results.append({\"lang\": lang, \"domain\": domain, **metrics})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db82691",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"law\", \"health\" ]\n",
    "\n",
    "languages = [ \"ar\", \"cs\", \"de\", \"el\", \"en\",\"es\", \"fa\", \"fr\", \"he\", \"hi\", \"id\", \"it\",\n",
    "             \"ja\" \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "results = []\n",
    "log_file_path = \"domain_language_lora_results.txt\"\n",
    "\n",
    "# EÄŸer varsa eski log dosyasÄ±nÄ± temizle\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "\n",
    "        print(f\"\\n=== {domain.upper()} - {lang.upper()} baÅŸlatÄ±lÄ±yor ===\")\n",
    "\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}-{domain} verisi yÃ¼klenemedi: {e}\")\n",
    "            continue\n",
    "\n",
    "        total = len(dataset)\n",
    "        train_data = dataset.select(range(0, int(total * 0.8)))\n",
    "        val_data   = dataset.select(range(int(total * 0.8), int(total * 0.9)))\n",
    "        test_data  = dataset.select(range(int(total * 0.9), total))\n",
    "\n",
    "        output_dir = f\"/home/enulu/Workspace/Domain_based/fine-tuned_modeller/{domain}/{lang}\"\n",
    "\n",
    "# Modelin eÄŸitilip eÄŸitilmediÄŸini kontrol et (Ã¶rneÄŸin: adapter_model.bin varsa eÄŸitilmiÅŸtir)\n",
    "        model_file = os.path.join(output_dir, \"vocab.json\")  # veya \"pytorch_model.bin\"\n",
    "\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"Model bulunamadÄ±, eÄŸitim baÅŸlatÄ±lÄ±yor: {output_dir}\")\n",
    "            with open(log_file_path, \"a\") as f:\n",
    "                f.write(f\"{lang}-{domain}: EÄŸitim baÅŸlatÄ±lÄ±yor\\n\")\n",
    "            fine_tune_model(train_data, val_data, output_dir, lang)\n",
    "        else:\n",
    "            print(f\"Model zaten mevcut, eÄŸitim atlanÄ±yor: {output_dir}\")\n",
    "            with open(log_file_path, \"a\") as f:\n",
    "                f.write(f\"{lang}-{domain}: EÄŸitim atlandÄ±, model mevcut\\n\")\n",
    "\n",
    "\n",
    "        # DeÄŸerlendirme\n",
    "        metrics = evaluate_model(output_dir, test_data, processor, lang_code=lang)\n",
    "        results.append({\n",
    "            \"domain\": domain,\n",
    "            \"lang\": lang,\n",
    "            **metrics\n",
    "        })\n",
    "                # AnlÄ±k log yaz\n",
    "        with open(log_file_path, \"a\") as f:\n",
    "            metric_line = f\"{domain}-{lang}: \" + \", \".join([f\"{k}={v:.4f}\" for k, v in metrics.items()])\n",
    "            f.write(metric_line + \"\\n\")\n",
    "\n",
    "# SonuÃ§larÄ± CSV'ye kaydet\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"domain_language_lora_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95d27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"law\", \"health\" ]\n",
    "\n",
    "languages = [ \"ar\", \"cs\", \"de\", \"el\", \"en\",\"es\", \"fa\", \"fr\", \"he\", \"hi\", \"id\", \"it\",\n",
    "             \"ja\" \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "results = []\n",
    "log_file_path = \"domain_language_lora_full-model.txt\"\n",
    "\n",
    "# EÄŸer varsa eski log dosyasÄ±nÄ± temizle\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "\n",
    "        print(f\"\\n=== {domain.upper()} - {lang.upper()} baÅŸlatÄ±lÄ±yor ===\")\n",
    "\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}-{domain} verisi yÃ¼klenemedi: {e}\")\n",
    "            continue\n",
    "\n",
    "        total = len(dataset)\n",
    "        train_data = dataset.select(range(0, int(total * 0.8)))\n",
    "        val_data   = dataset.select(range(int(total * 0.8), int(total * 0.9)))\n",
    "        test_data  = dataset.select(range(int(total * 0.9), total))\n",
    "\n",
    "        output_dir = f\"/home/enulu/Workspace/Domain_based/fine-tuned_modeller/fine-tuned-{domain}\"\n",
    "\n",
    "# Modelin eÄŸitilip eÄŸitilmediÄŸini kontrol et (Ã¶rneÄŸin: adapter_model.bin varsa eÄŸitilmiÅŸtir)\n",
    "        model_file = os.path.join(output_dir, \"adapter_config.json\")  # veya \"pytorch_model.bin\"\n",
    "\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"Model bulunamadÄ±, eÄŸitim baÅŸlatÄ±lÄ±yor: {output_dir}\")\n",
    "            with open(log_file_path, \"a\") as f:\n",
    "                f.write(f\"{lang}-{domain}: EÄŸitim baÅŸlatÄ±lÄ±yor\\n\")\n",
    "            fine_tune_model(train_data, val_data, output_dir, lang)\n",
    "        else:\n",
    "            print(f\"Model zaten mevcut, eÄŸitim atlanÄ±yor: {output_dir}\")\n",
    "            with open(log_file_path, \"a\") as f:\n",
    "                f.write(f\"{lang}-{domain}: EÄŸitim atlandÄ±, model mevcut\\n\")\n",
    "\n",
    "\n",
    "        # DeÄŸerlendirme\n",
    "        metrics = evaluate_model(output_dir, test_data, processor, lang_code=lang)\n",
    "        results.append({\n",
    "            \"domain\": domain,\n",
    "            \"lang\": lang,\n",
    "            **metrics\n",
    "        })\n",
    "                # AnlÄ±k log yaz\n",
    "        with open(log_file_path, \"a\") as f:\n",
    "            metric_line = f\"{domain}-{lang}: \" + \", \".join([f\"{k}={v:.4f}\" for k, v in metrics.items()])\n",
    "            f.write(metric_line + \"\\n\")\n",
    "\n",
    "# SonuÃ§larÄ± CSV'ye kaydet\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"domain_language_lora_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
