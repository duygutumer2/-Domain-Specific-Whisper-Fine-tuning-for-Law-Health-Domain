{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c51f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from huggingface_hub import login\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor,\n",
    "    WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training, LoraConfig,\n",
    "    get_peft_model, PeftModel, PeftConfig\n",
    ")\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fac270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. ENVIRONMENT & SETUP\n",
    "# ------------------------------------------------------------\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"None\"\n",
    "login(token=\"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\", add_to_git_credential=True)\n",
    "\n",
    "model_name_or_path = \"openai/whisper-large-v3\"\n",
    "task = \"transcribe\"\n",
    "#dataset_path = \"zh_health_dataset\"\n",
    "#sentence_counts = [1288, 416, 1843, 33, 3739, 3602, 1583, 2227, 58, 116, 177, 2574, 187, 13, 919, 276, 637, 130, 679, 666,556, 69, 516]\n",
    "\n",
    "#data = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8212415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. SPLIT DATASET BY LANGUAGE COUNTS\n",
    "# ------------------------------------------------------------\n",
    "def split_multilang_dataset(dataset, counts):\n",
    "    start_index = 0\n",
    "    train, test, eval = [], [], []\n",
    "    for count in counts:\n",
    "        end_index = start_index + count\n",
    "        if count != 1:\n",
    "            train.append(dataset.select(range(start_index, start_index + int(count * 0.8))))\n",
    "            test.append(dataset.select(range(start_index + int(count * 0.8), start_index + int(count * 0.9))))\n",
    "            eval.append(dataset.select(range(start_index + int(count * 0.9), start_index + count)))\n",
    "        else:\n",
    "            train.append(dataset.select(range(start_index, start_index + 1)))\n",
    "        start_index = end_index\n",
    "    return concatenate_datasets(train), concatenate_datasets(test), concatenate_datasets(eval)\n",
    "\n",
    "#train_data, test_data, eval_data = split_multilang_dataset(data, sentence_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3bc14e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enulu/anaconda3/envs/voice/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. PROCESSOR & COLLATOR\n",
    "# ------------------------------------------------------------\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, task=task)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features):\n",
    "        input_feats = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_feats, return_tensors=\"pt\")\n",
    "        label_feats = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_feats, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dba66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(train_data, val_data, output_dir, lang_code):\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, device_map=\"auto\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.model.encoder.conv1.register_forward_hook(lambda m, i, o: o.requires_grad_(True))\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05, bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1e-6,\n",
    "        warmup_steps=50,\n",
    "        eval_steps=100,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=3,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        generation_max_length=128,\n",
    "        logging_steps=1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    del trainer, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_from_encoded(model_path, dataset):\n",
    "    from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "    from datasets import load_from_disk\n",
    "    import torch\n",
    "    import jiwer\n",
    "    from jiwer import wer, Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Normalizer\n",
    "    nwer_norm = Compose([ToLowerCase(), RemovePunctuation(), RemoveMultipleSpaces(), Strip()])\n",
    "\n",
    "    print(f\"üîπ Veriset y√ºkleniyor: {dataset_path}\")\n",
    "    dataset = dataset\n",
    "\n",
    "    print(f\"üîπ Model y√ºkleniyor: {model_path}\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_path, device_map=\"auto\", )\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    all_predictions, all_references = [], []\n",
    "\n",
    "    print(\"üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\")\n",
    "    for sample in tqdm(dataset):\n",
    "        input_feats = torch.tensor(sample[\"input_features\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        try:\n",
    "            label_ids = [token for token in sample[\"labels\"] if token != -100]\n",
    "            reference = processor.tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Referans decode hatasƒ±: {e}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_ids = model.generate(input_feats)\n",
    "        prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        all_predictions.append(prediction)\n",
    "        all_references.append(reference)\n",
    "\n",
    "    overall_wer = wer(all_references, all_predictions)\n",
    "    normalized_references = [nwer_norm(r) for r in all_references]\n",
    "    normalized_predictions = [nwer_norm(p) for p in all_predictions]\n",
    "    overall_nwer = wer(normalized_references, normalized_predictions)\n",
    "\n",
    "    print(f\"\\n‚úÖ WER: {overall_wer:.4f}\")\n",
    "    print(f\"‚úÖ Normalized WER: {overall_nwer:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"WER\": overall_wer * 100,\n",
    "        \"Normalized_WER\": overall_nwer * 100\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6623814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ‚è≥ Evaluating: AR - LAW ===\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ar\n",
      "üîπ Model y√ºkleniyor: /home/enulu/Workspace/Domain_based/fine-tuned_modeller/law/ar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enulu/anaconda3/envs/voice/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/enulu/Workspace/Domain_based/fine-tuned_modeller/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# veya \"openai/whisper-large-v3\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_from_encoded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m deƒüerlendirme ba≈üarƒ±sƒ±z: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mevaluate_from_encoded\u001b[0;34m(model_path, dataset)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîπ Model y√ºkleniyor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m processor \u001b[38;5;241m=\u001b[39m WhisperProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-large-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     19\u001b[0m device \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/anaconda3/envs/voice/lib/python3.9/site-packages/transformers/modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3523\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3524\u001b[0m     (\n\u001b[1;32m   3525\u001b[0m         model,\n\u001b[1;32m   3526\u001b[0m         missing_keys,\n\u001b[1;32m   3527\u001b[0m         unexpected_keys,\n\u001b[1;32m   3528\u001b[0m         mismatched_keys,\n\u001b[1;32m   3529\u001b[0m         offload_index,\n\u001b[1;32m   3530\u001b[0m         error_msgs,\n\u001b[0;32m-> 3531\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3538\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3542\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3543\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3549\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/voice/lib/python3.9/site-packages/transformers/modeling_utils.py:3958\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3954\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   3955\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   3956\u001b[0m                 )\n\u001b[1;32m   3957\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3958\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3975\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/voice/lib/python3.9/site-packages/transformers/modeling_utils.py:778\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 778\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_param\u001b[38;5;241m.\u001b[39mis_contiguous():\n\u001b[1;32m    781\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "domains = [\"law\", \"health\"]\n",
    "languages = [\"ar\", \"cs\", \"de\", \"el\", \"en\", \"es\", \"fa\", \"fr\", \"he\", \"hi\", \"id\", \"it\",\n",
    "             \"ja\", \"ko\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "        print(f\"\\n=== ‚è≥ Evaluating: {lang.upper()} - {domain.upper()} ===\")\n",
    "\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}-{domain} verisi y√ºklenemedi: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Split dataset\n",
    "        total = len(dataset)\n",
    "        train_data = dataset.select(range(0, int(total * 0.8)))\n",
    "        eval_data = dataset.select(range(int(total * 0.8), int(total * 0.9)))\n",
    "        test_data = dataset.select(range(int(total * 0.9), total))\n",
    "\n",
    "\n",
    "\n",
    "        model_path = f\"/home/enulu/Workspace/Domain_based/fine-tuned_modeller/{domain}/{lang}\"  # veya \"openai/whisper-large-v3\"\n",
    "\n",
    "        try:\n",
    "            metrics = evaluate_from_encoded(model_path, test_data)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {lang}-{domain} deƒüerlendirme ba≈üarƒ±sƒ±z: {e}\")\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"domain\": domain,\n",
    "            \"lang\": lang,\n",
    "            **metrics\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5d8a90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Evaluating ORIGINAL Whisper: AR - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ar\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:24<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.5282\n",
      "‚úÖ Normalized WER: 0.4488\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: CS - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/cs\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:24<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1149\n",
      "‚úÖ Normalized WER: 0.0743\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: DE - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/de\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255/255 [02:22<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0401\n",
      "‚úÖ Normalized WER: 0.0215\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: EL - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/el\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1333\n",
      "‚úÖ Normalized WER: 0.0000\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: EN - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/en\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 387/387 [03:06<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1041\n",
      "‚úÖ Normalized WER: 0.0614\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: ES - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/es\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 273/273 [02:27<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0741\n",
      "‚úÖ Normalized WER: 0.0288\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: FA - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/fa\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [00:54<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.3393\n",
      "‚úÖ Normalized WER: 0.3016\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: FR - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/fr\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 298/298 [02:42<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1349\n",
      "‚úÖ Normalized WER: 0.0761\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: HE - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/he\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.4062\n",
      "‚úÖ Normalized WER: 0.2812\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: HI - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/hi\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.6286\n",
      "‚úÖ Normalized WER: 0.6286\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: ID - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/id\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:05<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0900\n",
      "‚úÖ Normalized WER: 0.0700\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: IT - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/it\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:53<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0883\n",
      "‚úÖ Normalized WER: 0.0431\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: JA - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ja\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:06<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.8182\n",
      "‚úÖ Normalized WER: 0.2727\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: KO - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ko\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.3333\n",
      "‚úÖ Normalized WER: 0.0000\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: NL - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/nl\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 195/195 [01:49<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0698\n",
      "‚úÖ Normalized WER: 0.0557\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: PL - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/pl\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:22<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1616\n",
      "‚úÖ Normalized WER: 0.0279\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: PT - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/pt\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:29<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.2500\n",
      "‚úÖ Normalized WER: 0.1148\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: RO - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ro\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:16<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0909\n",
      "‚úÖ Normalized WER: 0.0537\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: RU - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/ru\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:50<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0560\n",
      "‚úÖ Normalized WER: 0.0274\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: TR - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/tr\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:11<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1333\n",
      "‚úÖ Normalized WER: 0.0963\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: UK - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/uk\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:20<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.2103\n",
      "‚úÖ Normalized WER: 0.1536\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: VI - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/vi\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1429\n",
      "‚úÖ Normalized WER: 0.0000\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: ZH - LAW\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/law/zh\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:16<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 1.0357\n",
      "‚úÖ Normalized WER: 0.7143\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: AR - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ar\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [01:00<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.4150\n",
      "‚úÖ Normalized WER: 0.2912\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: CS - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/cs\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:23<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1049\n",
      "‚úÖ Normalized WER: 0.0839\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: DE - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/de\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 185/185 [01:36<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0561\n",
      "‚úÖ Normalized WER: 0.0376\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: EL - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/el\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1471\n",
      "‚úÖ Normalized WER: 0.0294\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: EN - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/en\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 374/374 [03:03<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1944\n",
      "‚úÖ Normalized WER: 0.1721\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: ES - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/es\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 361/361 [03:02<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0520\n",
      "‚úÖ Normalized WER: 0.0236\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: FA - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/fa\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 159/159 [01:27<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.3266\n",
      "‚úÖ Normalized WER: 0.2911\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: FR - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/fr\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 223/223 [01:55<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0996\n",
      "‚úÖ Normalized WER: 0.0608\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: HE - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/he\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:03<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.2174\n",
      "‚úÖ Normalized WER: 0.0870\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: HI - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/hi\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:10<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.2336\n",
      "‚úÖ Normalized WER: 0.2150\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: ID - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/id\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:10<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1062\n",
      "‚úÖ Normalized WER: 0.0750\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: IT - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/it\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 258/258 [02:24<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0612\n",
      "‚úÖ Normalized WER: 0.0357\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: JA - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ja\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 1.0000\n",
      "‚úÖ Normalized WER: 0.6316\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: KO - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ko\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.2000\n",
      "‚úÖ Normalized WER: 0.2000\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: NL - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/nl\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92/92 [00:51<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0680\n",
      "‚úÖ Normalized WER: 0.0592\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: PL - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/pl\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:16<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1783\n",
      "‚úÖ Normalized WER: 0.0306\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: PT - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/pt\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:31<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1937\n",
      "‚úÖ Normalized WER: 0.0631\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: RO - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ro\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:07<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.2143\n",
      "‚úÖ Normalized WER: 0.1531\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: RU - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/ru\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:39<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.0794\n",
      "‚úÖ Normalized WER: 0.0468\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: TR - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/tr\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [00:33<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1716\n",
      "‚úÖ Normalized WER: 0.1099\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: UK - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/uk\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.2129\n",
      "‚úÖ Normalized WER: 0.1639\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: VI - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/vi\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:03<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.1343\n",
      "‚úÖ Normalized WER: 0.0149\n",
      "\n",
      "üåç Evaluating ORIGINAL Whisper: ZH - HEALTH\n",
      "üîπ Veriset y√ºkleniyor: /home/enulu/Workspace/Domain_based/Dataset/health/zh\n",
      "üîπ Model y√ºkleniyor: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Deƒüerlendirme ba≈ülatƒ±ldƒ±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [00:31<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ WER: 0.9245\n",
      "‚úÖ Normalized WER: 0.7170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "original_results = []\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "        print(f\"\\nüåç Evaluating ORIGINAL Whisper: {lang.upper()} - {domain.upper()}\")\n",
    "\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        model_path = \"openai/whisper-large-v3\"  # Orijinal Hugging Face modeli\n",
    "\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "            dataset = dataset.shuffle(seed=42)\n",
    "            total = len(dataset)\n",
    "            test_data = dataset.select(range(int(0.9 * total), total))\n",
    "\n",
    "\n",
    "\n",
    "            metrics = evaluate_from_encoded(model_path, test_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[‚ùå ERROR] {lang}-{domain} (original): {e}\")\n",
    "            continue\n",
    "\n",
    "        original_results.append({\n",
    "            \"domain\": domain,\n",
    "            \"lang\": lang,\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "# CSV olarak orijinal sonu√ßlarƒ± kaydet\n",
    "df_orig = pd.DataFrame(original_results)\n",
    "df_orig.to_csv(\"original_whisper_eval_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac89475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"eval_result_latest.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6. EVALUATION\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_model(model_id, dataset, processor, lang_code, task=\"transcribe\"):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned Whisper model with WER and normalized WER.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): Path to the fine-tuned model.\n",
    "        dataset (Dataset): Hugging Face dataset to evaluate.\n",
    "        processor: WhisperProcessor instance.\n",
    "        lang_code (str): ISO language code (e.g., \"tr\", \"en\", \"zh\").\n",
    "        task (str): \"transcribe\" or \"translate\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with WER and normalized WER.\n",
    "    \"\"\"\n",
    "    peft_config = PeftConfig.from_pretrained(model_id)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        peft_config.base_model_name_or_path, device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model, model_id)\n",
    "    model.eval()\n",
    "\n",
    "    normalizer = BasicTextNormalizer()\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    dataloader = DataLoader(dataset, batch_size=5, collate_fn=data_collator)\n",
    "\n",
    "    # Try assigning forced_decoder_ids based on language\n",
    "    try:\n",
    "        forced_decoder_ids = processor.get_decoder_prompt_ids(language=lang_code, task=task)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] {lang_code} i√ßin forced_decoder_ids kullanƒ±lamadƒ±: {e}\")\n",
    "        forced_decoder_ids = None\n",
    "\n",
    "    preds, refs, norm_preds, norm_refs = [], [], [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Evaluating {lang_code}\"):\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            input_features = batch[\"input_features\"].to(\"cuda\")\n",
    "\n",
    "            generated = model.generate(\n",
    "                input_features=input_features,\n",
    "                forced_decoder_ids=forced_decoder_ids,\n",
    "                max_new_tokens=255\n",
    "            )\n",
    "\n",
    "            labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "            decoded_preds = processor.tokenizer.batch_decode(\n",
    "                generated.cpu().numpy(), skip_special_tokens=True)\n",
    "            decoded_labels = processor.tokenizer.batch_decode(\n",
    "                np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), processor.tokenizer.pad_token_id),\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            preds.extend(decoded_preds)\n",
    "            refs.extend(decoded_labels)\n",
    "            norm_preds.extend([normalizer(p).strip() for p in decoded_preds])\n",
    "            norm_refs.extend([normalizer(r).strip() for r in decoded_labels])\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=preds, references=refs)\n",
    "    norm_wer = 100 * metric.compute(predictions=norm_preds, references=norm_refs)\n",
    "\n",
    "    print(f\"[RESULT] {lang_code} WER: {wer:.2f}, Normalized WER: {norm_wer:.2f}\")\n",
    "    return {\"WER\": wer, \"Normalized_WER\": norm_wer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b77fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"domains = [\"law\", \"health\"]\n",
    "languages = [\"ar\", \"cs\", \"de\", \"el\", \"en\", \"es\", \"fa\", \"fr\", \"he\", \"hi\", \"id\", \"it\",\n",
    "             \"ja\", \"ko\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "        # Load dataset\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}-{domain} verisi y√ºklenemedi: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Split dataset\n",
    "        total = len(dataset)\n",
    "        train_data = dataset.select(range(0, int(total * 0.8)))\n",
    "        eval_data = dataset.select(range(int(total * 0.8), int(total * 0.9)))\n",
    "        test_data = dataset.select(range(int(total * 0.9), total))\n",
    "\n",
    "        # Fine-tune model (veya √∂nceden eƒüitilmi≈ü modeli y√ºkle)\n",
    "        model_path = f\"/home/enulu/Workspace/Domain_based/fine-tuned_modeller/fine-tuned-{domain}\"\n",
    "        if not os.path.exists(model_path):\n",
    "            fine_tune_model(train_data, eval_data, model_path, lang)\n",
    "\n",
    "        # Evaluate\n",
    "        metrics = evaluate_model(model_path, test_data, processor, lang)\n",
    "        print(f\"{lang}-{domain} WER:\", metrics)\n",
    "        results.append({\"lang\": lang, \"domain\": domain, **metrics})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db82691",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"law\", \"health\" ]\n",
    "\n",
    "languages = [ \"ar\", \"cs\", \"de\", \"el\", \"en\",\"es\", \"fa\", \"fr\", \"he\", \"hi\", \"id\", \"it\",\n",
    "             \"ja\" \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "results = []\n",
    "log_file_path = \"domain_language_lora_results.txt\"\n",
    "\n",
    "# Eƒüer varsa eski log dosyasƒ±nƒ± temizle\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "\n",
    "        print(f\"\\n=== {domain.upper()} - {lang.upper()} ba≈ülatƒ±lƒ±yor ===\")\n",
    "\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}-{domain} verisi y√ºklenemedi: {e}\")\n",
    "            continue\n",
    "\n",
    "        total = len(dataset)\n",
    "        train_data = dataset.select(range(0, int(total * 0.8)))\n",
    "        val_data   = dataset.select(range(int(total * 0.8), int(total * 0.9)))\n",
    "        test_data  = dataset.select(range(int(total * 0.9), total))\n",
    "\n",
    "        output_dir = f\"/home/enulu/Workspace/Domain_based/fine-tuned_modeller/{domain}/{lang}\"\n",
    "\n",
    "# Modelin eƒüitilip eƒüitilmediƒüini kontrol et (√∂rneƒüin: adapter_model.bin varsa eƒüitilmi≈ütir)\n",
    "        model_file = os.path.join(output_dir, \"vocab.json\")  # veya \"pytorch_model.bin\"\n",
    "\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"Model bulunamadƒ±, eƒüitim ba≈ülatƒ±lƒ±yor: {output_dir}\")\n",
    "            with open(log_file_path, \"a\") as f:\n",
    "                f.write(f\"{lang}-{domain}: Eƒüitim ba≈ülatƒ±lƒ±yor\\n\")\n",
    "            fine_tune_model(train_data, val_data, output_dir, lang)\n",
    "        else:\n",
    "            print(f\"Model zaten mevcut, eƒüitim atlanƒ±yor: {output_dir}\")\n",
    "            with open(log_file_path, \"a\") as f:\n",
    "                f.write(f\"{lang}-{domain}: Eƒüitim atlandƒ±, model mevcut\\n\")\n",
    "\n",
    "\n",
    "        # Deƒüerlendirme\n",
    "        metrics = evaluate_model(output_dir, test_data, processor, lang_code=lang)\n",
    "        results.append({\n",
    "            \"domain\": domain,\n",
    "            \"lang\": lang,\n",
    "            **metrics\n",
    "        })\n",
    "                # Anlƒ±k log yaz\n",
    "        with open(log_file_path, \"a\") as f:\n",
    "            metric_line = f\"{domain}-{lang}: \" + \", \".join([f\"{k}={v:.4f}\" for k, v in metrics.items()])\n",
    "            f.write(metric_line + \"\\n\")\n",
    "\n",
    "# Sonu√ßlarƒ± CSV'ye kaydet\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"domain_language_lora_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95d27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\"law\", \"health\" ]\n",
    "\n",
    "languages = [ \"ar\", \"cs\", \"de\", \"el\", \"en\",\"es\", \"fa\", \"fr\", \"he\", \"hi\", \"id\", \"it\",\n",
    "             \"ja\" \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"tr\", \"uk\", \"vi\", \"zh\"]\n",
    "\n",
    "results = []\n",
    "log_file_path = \"domain_language_lora_full-model.txt\"\n",
    "\n",
    "# Eƒüer varsa eski log dosyasƒ±nƒ± temizle\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "\n",
    "for domain in domains:\n",
    "    for lang in languages:\n",
    "\n",
    "        print(f\"\\n=== {domain.upper()} - {lang.upper()} ba≈ülatƒ±lƒ±yor ===\")\n",
    "\n",
    "        dataset_path = f\"/home/enulu/Workspace/Domain_based/Dataset/{domain}/{lang}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"{lang}-{domain} verisi y√ºklenemedi: {e}\")\n",
    "            continue\n",
    "\n",
    "        total = len(dataset)\n",
    "        train_data = dataset.select(range(0, int(total * 0.8)))\n",
    "        val_data   = dataset.select(range(int(total * 0.8), int(total * 0.9)))\n",
    "        test_data  = dataset.select(range(int(total * 0.9), total))\n",
    "\n",
    "        output_dir = f\"/home/enulu/Workspace/Domain_based/fine-tuned_modeller/fine-tuned-{domain}\"\n",
    "\n",
    "# Modelin eƒüitilip eƒüitilmediƒüini kontrol et (√∂rneƒüin: adapter_model.bin varsa eƒüitilmi≈ütir)\n",
    "        model_file = os.path.join(output_dir, \"adapter_config.json\")  # veya \"pytorch_model.bin\"\n",
    "\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"Model bulunamadƒ±, eƒüitim ba≈ülatƒ±lƒ±yor: {output_dir}\")\n",
    "            with open(log_file_path, \"a\") as f:\n",
    "                f.write(f\"{lang}-{domain}: Eƒüitim ba≈ülatƒ±lƒ±yor\\n\")\n",
    "            fine_tune_model(train_data, val_data, output_dir, lang)\n",
    "        else:\n",
    "            print(f\"Model zaten mevcut, eƒüitim atlanƒ±yor: {output_dir}\")\n",
    "            with open(log_file_path, \"a\") as f:\n",
    "                f.write(f\"{lang}-{domain}: Eƒüitim atlandƒ±, model mevcut\\n\")\n",
    "\n",
    "\n",
    "        # Deƒüerlendirme\n",
    "        metrics = evaluate_model(output_dir, test_data, processor, lang_code=lang)\n",
    "        results.append({\n",
    "            \"domain\": domain,\n",
    "            \"lang\": lang,\n",
    "            **metrics\n",
    "        })\n",
    "                # Anlƒ±k log yaz\n",
    "        with open(log_file_path, \"a\") as f:\n",
    "            metric_line = f\"{domain}-{lang}: \" + \", \".join([f\"{k}={v:.4f}\" for k, v in metrics.items()])\n",
    "            f.write(metric_line + \"\\n\")\n",
    "\n",
    "# Sonu√ßlarƒ± CSV'ye kaydet\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"domain_language_lora_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
