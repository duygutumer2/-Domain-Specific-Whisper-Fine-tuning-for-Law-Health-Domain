{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9056009,"sourceType":"datasetVersion","datasetId":5460513},{"sourceId":9056027,"sourceType":"datasetVersion","datasetId":5460525},{"sourceId":9056034,"sourceType":"datasetVersion","datasetId":5460530},{"sourceId":9056042,"sourceType":"datasetVersion","datasetId":5460536},{"sourceId":9056371,"sourceType":"datasetVersion","datasetId":5460767},{"sourceId":9056498,"sourceType":"datasetVersion","datasetId":5460858},{"sourceId":9056558,"sourceType":"datasetVersion","datasetId":5460903},{"sourceId":9056599,"sourceType":"datasetVersion","datasetId":5460929},{"sourceId":9056619,"sourceType":"datasetVersion","datasetId":5460943},{"sourceId":9057150,"sourceType":"datasetVersion","datasetId":5461355},{"sourceId":9057236,"sourceType":"datasetVersion","datasetId":5461420},{"sourceId":9057509,"sourceType":"datasetVersion","datasetId":5461621},{"sourceId":9057538,"sourceType":"datasetVersion","datasetId":5461644},{"sourceId":9057549,"sourceType":"datasetVersion","datasetId":5461653},{"sourceId":9057555,"sourceType":"datasetVersion","datasetId":5461657},{"sourceId":9057573,"sourceType":"datasetVersion","datasetId":5461673},{"sourceId":9057590,"sourceType":"datasetVersion","datasetId":5461687},{"sourceId":9057600,"sourceType":"datasetVersion","datasetId":5461695},{"sourceId":9057625,"sourceType":"datasetVersion","datasetId":5461713},{"sourceId":9057640,"sourceType":"datasetVersion","datasetId":5461727},{"sourceId":9057688,"sourceType":"datasetVersion","datasetId":5461765},{"sourceId":9057708,"sourceType":"datasetVersion","datasetId":5461782},{"sourceId":9057712,"sourceType":"datasetVersion","datasetId":5461786},{"sourceId":9058347,"sourceType":"datasetVersion","datasetId":5462283},{"sourceId":9058516,"sourceType":"datasetVersion","datasetId":5462418},{"sourceId":9086907,"sourceType":"datasetVersion","datasetId":5482899},{"sourceId":9087612,"sourceType":"datasetVersion","datasetId":5483423},{"sourceId":9088588,"sourceType":"datasetVersion","datasetId":5484137},{"sourceId":9088623,"sourceType":"datasetVersion","datasetId":5484166},{"sourceId":9094668,"sourceType":"datasetVersion","datasetId":5488389},{"sourceId":9094865,"sourceType":"datasetVersion","datasetId":5488538},{"sourceId":9097582,"sourceType":"datasetVersion","datasetId":5490468},{"sourceId":9097622,"sourceType":"datasetVersion","datasetId":5490490},{"sourceId":9097635,"sourceType":"datasetVersion","datasetId":5490496},{"sourceId":9097654,"sourceType":"datasetVersion","datasetId":5490506},{"sourceId":9097672,"sourceType":"datasetVersion","datasetId":5490515},{"sourceId":9097695,"sourceType":"datasetVersion","datasetId":5490529},{"sourceId":9097738,"sourceType":"datasetVersion","datasetId":5490548},{"sourceId":9098104,"sourceType":"datasetVersion","datasetId":5490757},{"sourceId":9098162,"sourceType":"datasetVersion","datasetId":5490785},{"sourceId":9098203,"sourceType":"datasetVersion","datasetId":5490805},{"sourceId":9110511,"sourceType":"datasetVersion","datasetId":5498734},{"sourceId":9110637,"sourceType":"datasetVersion","datasetId":5498813},{"sourceId":9110660,"sourceType":"datasetVersion","datasetId":5498830},{"sourceId":9110680,"sourceType":"datasetVersion","datasetId":5498844},{"sourceId":9110738,"sourceType":"datasetVersion","datasetId":5498886}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate accelerate bitsandbytes peft dataset datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T19:34:55.062494Z","iopub.execute_input":"2025-06-29T19:34:55.062778Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nCollecting peft\n  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\nCollecting dataset\n  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nCollecting torch>=1.10.0 (from accelerate)\n  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.42.3)\nCollecting huggingface-hub>=0.7.0 (from evaluate)\n  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\nCollecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: alembic>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from dataset) (1.13.2)\nCollecting banal>=1.0.1 (from dataset)\n  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (4.9.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nCollecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.7.0->evaluate)\n  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.3)\nCollecting typing-extensions>=4 (from alembic>=0.6.2->dataset)\n  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting sympy>=1.13.3 (from torch>=1.10.0->accelerate)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.6.4.1 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch>=1.10.0->accelerate)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.3.1 (from torch>=1.10.0->accelerate)\n  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.10/site-packages (from triton==3.3.1->torch>=1.10.0->accelerate) (69.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nDownloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\nDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\nDownloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, banal, typing-extensions, triton, sympy, sqlalchemy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-xet, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, dataset, torch, evaluate, bitsandbytes, peft\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Uninstalling typing_extensions-4.9.0:\n      Successfully uninstalled typing_extensions-4.9.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.0\n    Uninstalling sympy-1.13.0:\n      Successfully uninstalled sympy-1.13.0\n  Attempting uninstall: sqlalchemy\n    Found existing installation: SQLAlchemy 2.0.25\n    Uninstalling SQLAlchemy-2.0.25:\n      Successfully uninstalled SQLAlchemy-2.0.25\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.4\n    Uninstalling huggingface-hub-0.23.4:\n      Successfully uninstalled huggingface-hub-0.23.4\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport zlib\nfrom huggingface_hub import login\nfrom datasets import load_dataset, DatasetDict, Dataset\nfrom transformers import WhisperFeatureExtractor\nfrom transformers import WhisperTokenizer\nfrom transformers import WhisperProcessor\nfrom transformers import Seq2SeqTrainingArguments\nfrom datasets import Audio\nimport evaluate\nfrom peft import prepare_model_for_kbit_training\nfrom peft import PeftModel, PeftConfig\nfrom transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\nfrom peft import LoraConfig, get_peft_model\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport gc\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom transformers.models.whisper.english_normalizer import BasicTextNormalizer\nimport torch\nimport shutil\nimport subprocess\n#from langdetect import detect\nimport subprocess\nfrom huggingface_hub import scan_cache_dir\n\nos.environ[\"HF_DATASETS_CACHE\"] = \"None\"\n\nlogin(token = \"HF_TOKEN\",add_to_git_credential=True)\n\nmodel_name_or_path = \"openai/whisper-large-v3\"\n#model_name_or_path=\"devasheeshG/whisper_large_v2_fp16_transformers\"\n\n#model_name_or_path=\"airesearch/wav2vec2-large-xlsr-53-th\"\ntask = \"transcribe\"\ndataset_name = \"mozilla-foundation/common_voice_17_0\"\nlanguage = \"Turkish\"\nlanguage_abbr = \"tr\"\n\n\nlaw = DatasetDict()\nmed = DatasetDict()\nfinance = DatasetDict()\nsport = DatasetDict()\nother = DatasetDict()\n\ndomain_name = \"/kaggle/input/health-ru/health_ru\"#değişcek (law.txt)\n\ndef compress_dict(d):\n    return zlib.compress(pickle.dumps(d))\n\n# Function to decompress a dictionary\ndef decompress_dict(c):\n    return pickle.loads(zlib.decompress(c))\nlaw_list = list()\n\nwith open(f\"{domain_name}\", 'r', encoding='utf-8') as file:\n    for line in file:\n        law_list.append(line)\nprint(len(law_list))\n'''\nlanguages = ['ar', 'cs', 'de', 'el',\n             'en', 'es', 'fa', 'fr', 'he', 'hi',\n             'id', 'it', 'ja', 'ko',\n             'nl', 'pl', 'pt',\n             'ro', 'ru',\n             'tr', 'uk', 'vi', 'zh-CN']\n             '''\nlanguages=[\"ru\"]\n\ncommon_voice = DatasetDict()\nlaw_sentence = law_list[0]\nprint(law_sentence)\nindex2 = 0\ndictionaries = []\nfor i in tqdm(languages):\n    common_voice = load_dataset(\"mozilla-foundation/common_voice_17_0\", i, split=\"train\", use_auth_token=True, \n                                trust_remote_code=True, streaming=True)\n    index = 0\n    for example in common_voice:\n        sentence = example[\"sentence\"]\n        if i == \"en\" and index >= 113800:\n            break\n        if i == \"de\" and index >= 121200:\n            break\n        if i == \"es\" and index >= 114500:\n            break\n        if i == \"it\" and index >= 131819:\n            break\n        if i == \"fr\" and index >= 128200:\n            break\n\n        if sentence == law_sentence.replace(\" \\n\",\"\"):\n            dictionaries.append(example)\n            if len(law_list) > index2 + 1:\n                index2 = index2 + 1\n                law_sentence = law_list[index2]\n\n        if index % 1000 == 0:\n            print(\"you \" + str(index))\n        index = index + 1\n\n\nprint(\"LEN: \" + str(len(dictionaries)))\n\n\ndel common_voice\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:59:49.339051Z","iopub.execute_input":"2024-08-03T18:59:49.339506Z","iopub.status.idle":"2024-08-03T19:05:33.373367Z","shell.execute_reply.started":"2024-08-03T18:59:49.339473Z","shell.execute_reply":"2024-08-03T19:05:33.372303Z"},"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Token is valid (permission: read).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n680\nВ наших планах мы должны сделать особый упор на вопросе здоровья женщин и девочек. \n\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\nYou can remove this warning by passing 'token=<use_auth_token>' instead.\n  warnings.warn(\n\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 1it [00:00,  1.39it/s]\u001b[A\nReading metadata...: 9730it [00:00, 16113.07it/s]\u001b[A\nReading metadata...: 15453it [00:01, 10132.23it/s]\u001b[A\nReading metadata...: 26377it [00:01, 15032.01it/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"you 0\nhey\nhey\nhey\nhey\nhey\nyou 1000\nhey\nyou 2000\nhey\nhey\nyou 3000\nhey\nhey\nhey\nhey\nhey\nyou 4000\nhey\nhey\nyou 5000\nhey\nhey\nhey\nhey\nhey\nyou 6000\nhey\nhey\nhey\nyou 7000\nhey\nhey\nhey\nyou 8000\nhey\nhey\nhey\nhey\nhey\nhey\nhey\nyou 9000\nhey\nhey\nhey\nyou 10000\nhey\nhey\nhey\nhey\nyou 11000\nhey\nyou 12000\nhey\nhey\nhey\nyou 13000\nhey\nhey\nhey\nhey\nyou 14000\nhey\nhey\nhey\nhey\nhey\nyou 15000\nhey\nhey\nyou 16000\nhey\nhey\nyou 17000\nhey\nhey\nyou 18000\nhey\nhey\nyou 19000\nhey\nhey\nhey\nyou 20000\nhey\nhey\nhey\nyou 21000\nhey\nhey\nhey\nhey\nhey\nyou 22000\nhey\nyou 23000\nhey\nhey\nhey\nhey\nhey\nyou 24000\nhey\nyou 25000\nhey\nhey\nhey\nhey\nyou 26000\nhey\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [05:43<00:00, 343.49s/it]","output_type":"stream"},{"name":"stdout","text":"LEN: 679\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"19"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-07-31T06:18:06.392036Z","iopub.execute_input":"2024-07-31T06:18:06.393079Z","iopub.status.idle":"2024-07-31T06:18:22.034700Z","shell.execute_reply.started":"2024-07-31T06:18:06.393026Z","shell.execute_reply":"2024-07-31T06:18:22.033366Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.42.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def gen():\n    for d in dictionaries[0:400]:\n        yield d\n        \ndef gen2():\n    for d in dictionaries[400:]:\n        yield d\n        \n\"\"\"def gen3():\n    for d in dictionaries[800:1200]:\n        yield d\n        \ndef gen4():\n    for d in dictionaries[1200:1600]:\n        yield d\n        \ndef gen5():\n    for d in dictionaries[1600:2000]:\n        yield d\n        \ndef gen6():\n    for d in dictionaries[2000:2400]:\n        yield d\n        \ndef gen7():\n    for d in dictionaries[2400:]:\n        yield d\"\"\"\n\"\"\"\ndef gen8():\n    for d in dictionaries[2800:3200]:\n        yield d\ndef gen9():\n    for d in dictionaries[3200:3600]:\n        yield d\ndef gen10():\n    for d in dictionaries[3600:]:\n        yield d\n\"\"\"\nprint(len(dictionaries))\nlaw_data = Dataset.from_generator(gen)\nlaw_data = law_data.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\n\nlaw_data2 = Dataset.from_generator(gen2)\nlaw_data2 = law_data2.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\"\"\"\nlaw_data3 = Dataset.from_generator(gen3)\nlaw_data3 = law_data3.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\nlaw_data4 = Dataset.from_generator(gen4)\nlaw_data4 = law_data4.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\nlaw_data5 = Dataset.from_generator(gen5)\nlaw_data5 = law_data5.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\nlaw_data6 = Dataset.from_generator(gen6)\nlaw_data6 = law_data6.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\n\nlaw_data7 = Dataset.from_generator(gen7)\nlaw_data7 = law_data7.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\"\"\"\"\"\"\nlaw_data8 = Dataset.from_generator(gen8)\nlaw_data8 = law_data8.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\nlaw_data9 = Dataset.from_generator(gen9)\nlaw_data9 = law_data9.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\nlaw_data10 = Dataset.from_generator(gen10)\nlaw_data10 = law_data10.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n\"\"\"\n\"\"\"train_data = law_data[10:251]\n#test_data = law_data[0:100]\neval_data = law_data[:10]\"\"\"\n\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n\ntokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n\n\"\"\"processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n\"\"\"\n\nlaw_data = law_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nlaw_data2 = law_data2.cast_column(\"audio\", Audio(sampling_rate=16000))\n\"\"\"law_data3 = law_data3.cast_column(\"audio\", Audio(sampling_rate=16000))\n\nlaw_data4 = law_data4.cast_column(\"audio\", Audio(sampling_rate=16000))\nlaw_data5 = law_data5.cast_column(\"audio\", Audio(sampling_rate=16000))\nlaw_data6 = law_data6.cast_column(\"audio\", Audio(sampling_rate=16000))\n\nlaw_data7 = law_data7.cast_column(\"audio\", Audio(sampling_rate=16000))\"\"\"\n\"\"\"law_data8 = law_data8.cast_column(\"audio\", Audio(sampling_rate=16000))\nlaw_data9 = law_data9.cast_column(\"audio\", Audio(sampling_rate=16000))\nlaw_data10 = law_data10.cast_column(\"audio\", Audio(sampling_rate=16000))\"\"\"\n\ndef prepare_dataset(batch):\n    # load and resample audio data from 48 to 16kHz\n    audio = batch[\"audio\"]\n\n    # compute log-Mel input features from input audio array\n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n\n    # encode target text to label ids\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T19:05:46.021984Z","iopub.execute_input":"2024-08-03T19:05:46.022348Z","iopub.status.idle":"2024-08-03T19:08:03.269449Z","shell.execute_reply.started":"2024-08-03T19:05:46.022320Z","shell.execute_reply":"2024-08-03T19:08:03.268193Z"},"trusted":true},"outputs":[{"name":"stdout","text":"679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c56489f48577414d8f0bec3db6062c73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a37078dc4154f1cb9e5de5654d489f8"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"law_data6 = Dataset.from_generator(gen6)\nlaw_data6 = law_data6.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\nlaw_data6 = law_data6.cast_column(\"audio\", Audio(sampling_rate=16000))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:29:02.599116Z","iopub.execute_input":"2024-08-03T18:29:02.599660Z","iopub.status.idle":"2024-08-03T18:30:20.059879Z","shell.execute_reply.started":"2024-08-03T18:29:02.599615Z","shell.execute_reply":"2024-08-03T18:30:20.058138Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T10:37:54.504358Z","iopub.execute_input":"2024-07-29T10:37:54.504795Z","iopub.status.idle":"2024-07-29T10:37:54.510207Z","shell.execute_reply.started":"2024-07-29T10:37:54.504762Z","shell.execute_reply":"2024-07-29T10:37:54.508945Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:08:49.870326Z","iopub.execute_input":"2024-08-03T18:08:49.870762Z","iopub.status.idle":"2024-08-03T18:08:50.327199Z","shell.execute_reply.started":"2024-08-03T18:08:49.870727Z","shell.execute_reply":"2024-08-03T18:08:50.326118Z"},"trusted":true},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"law_data = law_data.map(prepare_dataset, remove_columns=law_data.column_names, num_proc=2)\nlaw_data.save_to_disk('/kaggle/working/ru_dataset1')\ndel law_data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T19:08:34.500079Z","iopub.execute_input":"2024-08-03T19:08:34.500868Z","iopub.status.idle":"2024-08-03T19:09:47.709498Z","shell.execute_reply.started":"2024-08-03T19:08:34.500834Z","shell.execute_reply":"2024-08-03T19:09:47.708500Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576ddece6db2411db684d319d5df98a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc511fb0b3314a2197c0649e30e4d2b9"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"165"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"law_data2 = law_data2.map(prepare_dataset, remove_columns=law_data2.column_names, num_proc=2)\nlaw_data2.save_to_disk('/kaggle/working/ru_dataset2')\ndel law_data2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T19:09:53.783083Z","iopub.execute_input":"2024-08-03T19:09:53.783530Z","iopub.status.idle":"2024-08-03T19:10:44.545028Z","shell.execute_reply.started":"2024-08-03T19:09:53.783494Z","shell.execute_reply":"2024-08-03T19:10:44.543756Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/279 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87c4a7174412440cb4f60132de0a044d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/279 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5a4f31a12e4e53bcda29730af9b412"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"140"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"law_data3 = law_data3.map(prepare_dataset, remove_columns=law_data3.column_names, num_proc=2)\nlaw_data3.save_to_disk('/kaggle/working/it_dataset3')\ndel law_data3\ngc.collect()\nlaw_data4 = law_data4.map(prepare_dataset, remove_columns=law_data4.column_names, num_proc=2)\nlaw_data4.save_to_disk('/kaggle/working/it_dataset4')\ndel law_data4\ngc.collect()\nlaw_data5 = law_data5.map(prepare_dataset, remove_columns=law_data5.column_names, num_proc=2)\nlaw_data5.save_to_disk('/kaggle/working/it_dataset5')\ndel law_data5\ngc.collect()\nlaw_data6 = law_data6.map(prepare_dataset, remove_columns=law_data6.column_names, num_proc=2)\nlaw_data6.save_to_disk('/kaggle/working/it_dataset6')\ndel law_data6\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:13:03.510333Z","iopub.execute_input":"2024-08-03T18:13:03.510846Z","iopub.status.idle":"2024-08-03T18:18:52.942682Z","shell.execute_reply.started":"2024-08-03T18:13:03.510804Z","shell.execute_reply":"2024-08-03T18:18:52.941344Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"642005d800e7451aa48accc018203282"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"762b067d799c4f7a939ec664df6d0a6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00d813def3f4abcb4606c664c0882fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b0d90ce0fbb4b00943fae5037dc92ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4984ac9619d148ab8bb7f5b696235578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86a704c41e10426e8ac5832ad3d25a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/574 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1fbeca3342e40bab81a7d872ba82367"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/574 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d14929adf34480b9e14ee04f59cb4f1"}},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"178"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"law_data6 = law_data6.map(prepare_dataset, remove_columns=law_data6.column_names, num_proc=2)\nlaw_data6.save_to_disk('/kaggle/working/it_dataset6')\ndel law_data6\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:30:34.825742Z","iopub.execute_input":"2024-08-03T18:30:34.827559Z","iopub.status.idle":"2024-08-03T18:32:03.366938Z","shell.execute_reply.started":"2024-08-03T18:30:34.827406Z","shell.execute_reply":"2024-08-03T18:32:03.365554Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f8f6bc896514c9e8f820a78af3f9cea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba53c9eb29044edacee7c04684a9212"}},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"1558"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"law_data7 = law_data7.map(prepare_dataset, remove_columns=law_data7.column_names, num_proc=2)\nlaw_data7.save_to_disk('/kaggle/working/it_dataset7')\ndel law_data7\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:19:32.718988Z","iopub.execute_input":"2024-08-03T18:19:32.719978Z","iopub.status.idle":"2024-08-03T18:20:10.819095Z","shell.execute_reply.started":"2024-08-03T18:19:32.719931Z","shell.execute_reply":"2024-08-03T18:20:10.817838Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/174 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4738616104744f6abf21195ba30621e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/174 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57444bbfe6e4d15977224c45136e567"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"140"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"law_data8 = law_data8.map(prepare_dataset, remove_columns=law_data8.column_names, num_proc=2)\nlaw_data8.save_to_disk('/kaggle/working/en_dataset8')\ndel law_data8\ngc.collect()\nlaw_data9 = law_data9.map(prepare_dataset, remove_columns=law_data9.column_names, num_proc=2)\nlaw_data9.save_to_disk('/kaggle/working/en_dataset9')\ndel law_data9\ngc.collect()\nlaw_data10 = law_data10.map(prepare_dataset, remove_columns=law_data10.column_names, num_proc=2)\nlaw_data10.save_to_disk('/kaggle/working/en_dataset10')\ndel law_data10\ngc.collect()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"law_data.save_to_disk('/kaggle/working/fa_dataset1')\nlaw_data2.save_to_disk('/kaggle/working/fa_dataset2')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nfrom datasets import load_from_disk\n\n# Load the Dataset from the directory\ndata1 = load_from_disk('/kaggle/input/ar-dataset1')\ndata2 = load_from_disk('/kaggle/input/cs-dataset/cs_dataset')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nfrom datasets import load_from_disk\n\n# Load the Dataset from the directory\ndata1 = load_from_disk('/kaggle/working/ru_dataset1')\ndata2 = load_from_disk('/kaggle/working/ru_dataset2')\n\"\"\"data3 = load_from_disk('/kaggle/working/it_dataset3')\ndata4 = load_from_disk('/kaggle/working/it_dataset4')\ndata5 = load_from_disk('/kaggle/working/it_dataset5')\ndata6 = load_from_disk('/kaggle/working/it_dataset6')\ndata7 = load_from_disk('/kaggle/working/it_dataset7')\"\"\"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T19:10:52.678742Z","iopub.execute_input":"2024-08-03T19:10:52.679668Z","iopub.status.idle":"2024-08-03T19:10:52.696551Z","shell.execute_reply.started":"2024-08-03T19:10:52.679627Z","shell.execute_reply":"2024-08-03T19:10:52.695289Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"data3 = load_from_disk('/kaggle/working/it_dataset3')\\ndata4 = load_from_disk('/kaggle/working/it_dataset4')\\ndata5 = load_from_disk('/kaggle/working/it_dataset5')\\ndata6 = load_from_disk('/kaggle/working/it_dataset6')\\ndata7 = load_from_disk('/kaggle/working/it_dataset7')\""},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import pickle\nfrom datasets import load_from_disk\n\nfrom datasets import concatenate_datasets\n\n# Concatenate the two Datasets\ncombined_dataset = concatenate_datasets([data1, data2])\ncombined_dataset.save_to_disk('/kaggle/working/ru_dataset_med')","metadata":{"execution":{"iopub.status.busy":"2024-08-03T19:10:58.262838Z","iopub.execute_input":"2024-08-03T19:10:58.263275Z","iopub.status.idle":"2024-08-03T19:10:59.906592Z","shell.execute_reply.started":"2024-08-03T19:10:58.263242Z","shell.execute_reply":"2024-08-03T19:10:59.905517Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/3 shards):   0%|          | 0/679 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1859bc6e09c34d678efc51503bb4f89e"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"data20 = load_from_disk('/kaggle/input/ru-dataset/ru_dataset')","metadata":{"execution":{"iopub.status.busy":"2024-07-31T06:31:50.528533Z","iopub.execute_input":"2024-07-31T06:31:50.529574Z","iopub.status.idle":"2024-07-31T06:31:50.560240Z","shell.execute_reply.started":"2024-07-31T06:31:50.529533Z","shell.execute_reply":"2024-07-31T06:31:50.559407Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data21 = load_from_disk('/kaggle/input/tr-dataset')\ndata22= load_from_disk('/kaggle/input/uk-dataset/uk_dataset')\ndata23= load_from_disk('/kaggle/input/vi-dataset/vi_dataset')\ndata24 = load_from_disk('/kaggle/input/zh-dataset/zh-dataset')","metadata":{"execution":{"iopub.status.busy":"2024-07-31T06:31:51.484090Z","iopub.execute_input":"2024-07-31T06:31:51.484497Z","iopub.status.idle":"2024-07-31T06:31:51.576270Z","shell.execute_reply.started":"2024-07-31T06:31:51.484442Z","shell.execute_reply":"2024-07-31T06:31:51.575350Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"for i in range(1,25):\n    name = f\"data{i}\"\n    print(globals()[name])  \n    \n    \"\"\"\n    531 518 0210\n    539 667 4439\n    532 646 1747\n    532 646 1746\n    530 602 4838\n    534 456 8110\n    533 654 1527\n    533 239 5740\n    530 095 0309\n    \n    \"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T13:08:26.780385Z","iopub.execute_input":"2024-07-30T13:08:26.780858Z","iopub.status.idle":"2024-07-30T13:08:26.788770Z","shell.execute_reply.started":"2024-07-30T13:08:26.780827Z","shell.execute_reply":"2024-07-30T13:08:26.787285Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_features', 'labels'],\n    num_rows: 470\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 365\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 2543\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 16\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 3869\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 2724\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 881\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 2450\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 529\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 38\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 43\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 89\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 1974\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 103\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 1\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 1942\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 355\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 551\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 334\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 870\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 251\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 361\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 8\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 274\n})\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!pip install evaluate peft jiwer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport zlib\nfrom huggingface_hub import login\nfrom datasets import load_dataset, DatasetDict, Dataset\nfrom transformers import WhisperFeatureExtractor\nfrom transformers import WhisperTokenizer\nfrom transformers import WhisperProcessor\nfrom transformers import Seq2SeqTrainingArguments\nfrom datasets import Audio\nimport evaluate\nfrom peft import prepare_model_for_kbit_training\nfrom peft import PeftModel, PeftConfig\nfrom transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\nfrom peft import LoraConfig, get_peft_model\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport gc\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom transformers.models.whisper.english_normalizer import BasicTextNormalizer\nimport torch\nimport shutil\nimport subprocess\n#from langdetect import detect\nimport subprocess\nfrom huggingface_hub import scan_cache_dir\nfrom transformers import WhisperTokenizer\n\nmodel_name_or_path = \"openai/whisper-large-v3\"\ntokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=\"ar\", task=\"transcribe\")","metadata":{"execution":{"iopub.status.busy":"2024-07-31T06:30:26.383268Z","iopub.execute_input":"2024-07-31T06:30:26.384136Z","iopub.status.idle":"2024-07-31T06:30:47.529391Z","shell.execute_reply.started":"2024-07-31T06:30:26.384100Z","shell.execute_reply":"2024-07-31T06:30:47.528344Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-07-31 06:30:35.202833: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-31 06:30:35.202952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-31 06:30:35.310918: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc6e41b8410041e298c55e09b58b9724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab0081204ca94b4ab375d320702729e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f8de044a1bc46678913c642ced65467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3af5a79c1924c0c81a989cecea9aa03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d4778aebf50468fb417bb14c4d78cb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb28423e0e2c49d9ba4527dc2a9c1c61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fbffe031bf245b4b1c6fa5be53a2606"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v3\")\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\", task=\"transcribe\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset, concatenate_datasets\n\n# Example combined dataset (replace this with your actual dataset)\n# combined_dataset = ...  # Load your actual dataset here\n\n# Sentence counts for each language\nsentence_counts = [470, 365, 2543, 16, 3869, 2724, 881, 2979, 38, 43, 89, 1974, 103, 1, 1942, 355, 551, 334, 870, 251, 361, 8, 274]\n\nnew_datasets_train = []\nnew_datasets_test = []\nnew_datasets_eval = []\n\nstart_index = 0\nfor count in sentence_counts:\n    end_index = start_index + count\n    last_20_percent_index = int(count * 0.8)  # Get the index for the last 20%\n    \n    # Calculate indices for splits\n    train_end_index = int(count * 0.8)  # First 80% for training\n    test_end_index = int(count * 0.9)   # Next 10% for testing\n    eval_end_index = count                # Remaining 10% for evaluation\n   \n    # Extract datasets for current language\n    if count != 1:\n        train_language_subset = combined_dataset.select(range(start_index, start_index+train_end_index))\n        test_language_subset = combined_dataset.select(range(start_index+int(count * 0.8), start_index+int(count * 0.9)))\n        eval_language_subset = combined_dataset.select(range(start_index+int(count * 0.9), start_index+count))\n        print(test_language_subset)\n        # Append to respective datasets\n        new_datasets_train.append(train_language_subset)\n        new_datasets_test.append(test_language_subset)\n        new_datasets_eval.append(eval_language_subset)\n    else:\n        new_datasets_train.append(combined_dataset.select(range(start_index,start_index+1)))\n    start_index = end_index\n\n# Concatenate all new datasets into one\nfinal_dataset_train = concatenate_datasets(new_datasets_train)\nfinal_dataset_test = concatenate_datasets(new_datasets_test)\nfinal_dataset_eval = concatenate_datasets(new_datasets_eval)\n\n# Save or use the final datasets as needed\nprint(\"Training dataset:\")\nprint(final_dataset_train)\nprint(\"Testing dataset:\")\nprint(final_dataset_test)\nprint(\"Evaluation dataset:\")\nprint(final_dataset_eval)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T07:23:35.923782Z","iopub.execute_input":"2024-07-31T07:23:35.924728Z","iopub.status.idle":"2024-07-31T07:23:36.159502Z","shell.execute_reply.started":"2024-07-31T07:23:35.924693Z","shell.execute_reply":"2024-07-31T07:23:36.158569Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_features', 'labels'],\n    num_rows: 47\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 36\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 254\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 2\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 387\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 272\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 88\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 298\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 4\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 4\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 9\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 197\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 10\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 194\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 35\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 55\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 33\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 87\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 25\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 36\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 1\n})\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 27\n})\nTraining dataset:\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 16825\n})\nTesting dataset:\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 2101\n})\nEvaluation dataset:\nDataset({\n    features: ['input_features', 'labels'],\n    num_rows: 2115\n})\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\nmetric = evaluate.load(\"wer\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T06:34:47.440922Z","iopub.execute_input":"2024-07-31T06:34:47.441336Z","iopub.status.idle":"2024-07-31T06:34:47.916698Z","shell.execute_reply.started":"2024-07-31T06:34:47.441305Z","shell.execute_reply":"2024-07-31T06:34:47.915604Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f46af8b1226e4f8b8d009964090bc687"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\", device_map=\"auto\")\nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []\n\n\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T06:35:12.815340Z","iopub.execute_input":"2024-07-31T06:35:12.815771Z","iopub.status.idle":"2024-07-31T06:35:33.815411Z","shell.execute_reply.started":"2024-07-31T06:35:12.815738Z","shell.execute_reply":"2024-07-31T06:35:33.814291Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff22d0465d84e6e9783418210df8397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57b50d83e7b94d52ba48fe5c888cd64d"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.utils.data import DataLoader\neval_dataloader = DataLoader(final_dataset_eval.select(range(2048,2049)), batch_size=5, collate_fn=data_collator)\nforced_decoder_ids = processor.get_decoder_prompt_ids(language = \"tr\", task=\"transcribe\")\nnormalizer = BasicTextNormalizer()\n\npredictions = []\nreferences = []\nnormalized_predictions = []\nnormalized_references = []\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.eval()\ni=0\nfor step, batch in enumerate(tqdm(eval_dataloader)):\n    with torch.cuda.amp.autocast():\n        with torch.no_grad():\n            #print(law_data_backup[\"sentence\"][i])\n            #lang_sentence = backup_law_data[\"sentence\"][i]\n            i = i + 1\n          \n            input_features = batch[\"input_features\"].to(device)\n            #forced_decoder_ids = correct_lang_decoder(lang_sentence)\n            generated_tokens = (\n                    model.generate(\n                    input_features=input_features,\n                    forced_decoder_ids=forced_decoder_ids,\n                    max_new_tokens=255,\n                )\n                .cpu()\n                .numpy()\n            )\n            labels = batch[\"labels\"].to(device)\n            labels = labels.cpu().numpy()\n            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n            print(decoded_preds)\n            print(decoded_labels)\n            predictions.extend(decoded_preds)\n            references.extend(decoded_labels)\n            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n        del generated_tokens, labels, batch, input_features\n    gc.collect()\nwer = 100 * metric.compute(predictions=predictions, references=references)\nnormalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\neval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n\nprint(f\"{wer=} and {normalized_wer=}\")\nprint(eval_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T07:02:21.795180Z","iopub.execute_input":"2024-07-31T07:02:21.796109Z","iopub.status.idle":"2024-07-31T07:02:26.203796Z","shell.execute_reply.started":"2024-07-31T07:02:21.796075Z","shell.execute_reply":"2024-07-31T07:02:26.202837Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:04<00:00,  4.37s/it]","output_type":"stream"},{"name":"stdout","text":"[' Bütçe kanun teklifi Cumhurbaşkanı tarafından hazırlanır ve meclis tarafından onaylanır.']\n['Bütçe kanun teklifi Cumhurbaşkanı tarafından hazırlanır ve Meclis tarafından onaylanır.']\nwer=10.0 and normalized_wer=0.0\n{'eval/wer': 10.0, 'eval/normalized_wer': 0.0}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from datasets import load_from_disk","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:31:09.755478Z","iopub.execute_input":"2024-08-15T08:31:09.755991Z","iopub.status.idle":"2024-08-15T08:31:11.835444Z","shell.execute_reply.started":"2024-08-15T08:31:09.755951Z","shell.execute_reply":"2024-08-15T08:31:11.833576Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data21 = load_from_disk('/kaggle/input/fr-dataset/fr_dataset')\ndata22= load_from_disk('/kaggle/input/fr-dataset-son')","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:31:36.781718Z","iopub.execute_input":"2024-08-15T08:31:36.782662Z","iopub.status.idle":"2024-08-15T08:31:36.903275Z","shell.execute_reply.started":"2024-08-15T08:31:36.782615Z","shell.execute_reply":"2024-08-15T08:31:36.902297Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\n# Concatenate the two Datasets\ncombined_dataset = concatenate_datasets([data21, data22])\ncombined_dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:31:39.452962Z","iopub.execute_input":"2024-08-15T08:31:39.453412Z","iopub.status.idle":"2024-08-15T08:31:39.472096Z","shell.execute_reply.started":"2024-08-15T08:31:39.453381Z","shell.execute_reply":"2024-08-15T08:31:39.470781Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_features', 'labels'],\n    num_rows: 2979\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\n# Concatenate the two Datasets\ncombined_dataset = concatenate_datasets([data21,data22])\n\nprint(combined_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:31:53.505245Z","iopub.execute_input":"2024-08-15T08:31:53.505674Z","iopub.status.idle":"2024-08-15T08:31:53.521308Z","shell.execute_reply.started":"2024-08-15T08:31:53.505639Z","shell.execute_reply":"2024-08-15T08:31:53.519997Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_features', 'labels'],\n    num_rows: 2979\n})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"combined_dataset.save_to_disk('/kaggle/working/fr_law_dataset')","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:32:17.469430Z","iopub.execute_input":"2024-08-15T08:32:17.469847Z","iopub.status.idle":"2024-08-15T08:33:18.593324Z","shell.execute_reply.started":"2024-08-15T08:32:17.469813Z","shell.execute_reply":"2024-08-15T08:33:18.583896Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/10 shards):   0%|          | 0/2979 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a279ba66e53547ad9853a655ec9920f6"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"!pip install jiwer","metadata":{"execution":{"iopub.status.busy":"2024-08-02T04:20:19.618516Z","iopub.execute_input":"2024-08-02T04:20:19.619396Z","iopub.status.idle":"2024-08-02T04:20:34.515839Z","shell.execute_reply.started":"2024-08-02T04:20:19.619365Z","shell.execute_reply":"2024-08-02T04:20:34.514819Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.9.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport pickle\nimport zlib\n\nimport datasets\nfrom huggingface_hub import login\nfrom datasets import load_dataset, DatasetDict, Dataset\nfrom transformers import WhisperFeatureExtractor\nfrom transformers import WhisperTokenizer\nfrom transformers import WhisperProcessor\nfrom transformers import Seq2SeqTrainingArguments\nfrom datasets import Audio\nimport evaluate\nfrom peft import prepare_model_for_kbit_training\nfrom peft import PeftModel, PeftConfig\nfrom transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\nfrom peft import LoraConfig, get_peft_model\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport gc\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom transformers.models.whisper.english_normalizer import BasicTextNormalizer\nimport torch\nfrom huggingface_hub import scan_cache_dir\nfrom sklearn.model_selection import train_test_split\n\nos.environ[\"HF_DATASETS_CACHE\"] = \"None\"\n\nlogin(token = \"HF_TOKEN\",add_to_git_credential=True)\n\nmodel_name_or_path = \"openai/whisper-large-v3\"\n#model_name_or_path = \"fine-tuned-ar\"\n#model_name_or_path = \"fine-tuned\"\n\n#model_name_or_path=\"devasheeshG/whisper_large_v2_fp16_transformers\"\n\n#model_name_or_path=\"airesearch/wav2vec2-large-xlsr-53-th\"\ntask = \"transcribe\"\n\nlaw_data = datasets.load_from_disk(\"/kaggle/input/pt-dataset/pt_dataset\")\n\nfrom datasets import Dataset, concatenate_datasets\n\n# Example combined dataset (replace this with your actual dataset)\n# combined_dataset = ...  # Load your actual dataset here\n\n# Sentence counts for each language\n#sentence_counts = [470, 365, 2543, 16, 3869, 2724, 881, 2979, 38, 43, 89, 1974, 103, 1, 1942, 355, 551, 334, 870, 251,361, 8, 274]\nsentence_counts = [551]\n\nnew_datasets_train = []\nnew_datasets_test = []\nnew_datasets_eval = []\n\nstart_index = 0\nfor count in sentence_counts:\n    end_index = start_index + count\n    last_20_percent_index = int(count * 0.8)  # Get the index for the last 20%\n\n    # Calculate indices for splits\n    train_end_index = int(count * 0.8)  # First 80% for training\n    test_end_index = int(count * 0.9)  # Next 10% for testing\n    eval_end_index = count  # Remaining 10% for evaluation\n\n    # Extract datasets for current language\n    if count != 1:\n        train_language_subset = law_data.select(range(start_index, start_index + train_end_index))\n        test_language_subset = law_data.select(\n            range(start_index + int(count * 0.8), start_index + int(count * 0.9)))\n        eval_language_subset = law_data.select(range(start_index + int(count * 0.9), start_index + count))\n        print(train_language_subset)\n        # Append to respective datasets\n        new_datasets_train.append(train_language_subset)\n        new_datasets_test.append(test_language_subset)\n        new_datasets_eval.append(eval_language_subset)\n    else:\n        new_datasets_train.append(law_data.select(range(start_index, start_index + 1)))\n    start_index = end_index\n\n# Concatenate all new datasets into one\ntrain_data = concatenate_datasets(new_datasets_train)\ntest_data = concatenate_datasets(new_datasets_test)\neval_data = concatenate_datasets(new_datasets_eval)\n\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n\ntokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, task=task)\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\", task=task)\n\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\nmetric = evaluate.load(\"wer\")\n\n\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit= True, device_map=\"auto\")\nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []\n\n\nmodel = prepare_model_for_kbit_training(model)\ndef make_inputs_require_grad(module, input, output):\n    output.requires_grad_(True)\n\nmodel.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)\n\nconfig = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n\nfrom transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"reach-vb/test\",\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-5,\n    warmup_steps=50,\n    eval_steps=100,  # Evaluate every 100 steps\n    gradient_checkpointing=True,\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    fp16=True,\n    report_to=\"none\",\n    per_device_eval_batch_size=8,\n    generation_max_length=128,\n    logging_steps=1,\n    lr_scheduler_type=\"linear\",\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n    load_best_model_at_end=True,\n    greater_is_better=False,\n)\n\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    data_collator=data_collator,\n    tokenizer=processor.feature_extractor,\n   # callbacks=[SavePeftModelCallback],\n)\ntorch.cuda.empty_cache()\n#model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n\ntrainer.model.save_pretrained(\"fine-tuned-pt\")\ntrainer.tokenizer.save_pretrained(\"fine-tuned-pt\")\n#'train_loss': 2.2751105626424155\n\"\"\"\n\n\n\n\"\"\"\n\"\"\"\nar only\nwer=41.84100418410041 and normalized_wer=43.36569579288026\n{'eval/wer': 41.84100418410041, 'eval/normalized_wer': 43.36569579288026}\n\ncs only\nwer=10.491803278688524 and normalized_wer=6.885245901639345\n{'eval/wer': 10.491803278688524, 'eval/normalized_wer': 6.885245901639345}\n\nde only\nwer=3.998334027488547 and normalized_wer=2.5758205234732032\n{'eval/wer': 3.998334027488547, 'eval/normalized_wer': 2.5758205234732032}\n\nel only\nwer=20.0 and normalized_wer=6.666666666666667\n{'eval/wer': 20.0, 'eval/normalized_wer': 6.666666666666667}\n\nen only\nwer=8.887130801687764 and normalized_wer=4.579755399427531\n{'eval/wer': 8.887130801687764, 'eval/normalized_wer': 4.579755399427531}\n\nes only\nwer=7.092980521866961 and normalized_wer=2.4623300257258363\n{'eval/wer': 7.092980521866961, 'eval/normalized_wer': 2.4623300257258363}\n\nfa only----????????????\nwer=25.04378283712785 and normalized_wer=21.015761821366024\n{'eval/wer': 25.04378283712785, 'eval/normalized_wer': 21.015761821366024}\n\nfr only\n\n************************fine-tuned******************************************\nar: \nwer=43.93305439330544 and normalized_wer=43.36569579288026\n{'eval/wer': 43.93305439330544, 'eval/normalized_wer': 43.36569579288026}\n\ncs:\nwer=10.819672131147541 and normalized_wer=7.540983606557377\n{'eval/wer': 10.819672131147541, 'eval/normalized_wer': 7.540983606557377}\n\nde:\nwer=4.539775093710953 and normalized_wer=2.9912754466140425\n{'eval/wer': 4.539775093710953, 'eval/normalized_wer': 2.9912754466140425}\n\nel:\nwer=20.0 and normalized_wer=6.666666666666667\n{'eval/wer': 20.0, 'eval/normalized_wer': 6.666666666666667}\n\nen:\nwer=8.966244725738397 and normalized_wer=4.657819411917773\n{'eval/wer': 8.966244725738397, 'eval/normalized_wer': 4.657819411917773}\n\nes:\nwer=7.239985299522235 and normalized_wer=2.499081220139655\n{'eval/wer': 7.239985299522235, 'eval/normalized_wer': 2.499081220139655}\n\nfa:\nwer=21.541155866900176 and normalized_wer=19.614711033274958\n{'eval/wer': 21.541155866900176, 'eval/normalized_wer': 19.614711033274958}\n\nfr:\nwer=13.718291054739653 and normalized_wer=9.309590752889722\n{'eval/wer': 13.718291054739653, 'eval/normalized_wer': 9.309590752889722}\n\nhe:\nwer=28.205128205128204 and normalized_wer=17.94871794871795\n{'eval/wer': 28.205128205128204, 'eval/normalized_wer': 17.94871794871795}\n\nhi:\nwer=27.586206896551722 and normalized_wer=4.081632653061225\n{'eval/wer': 27.586206896551722, 'eval/normalized_wer': 4.081632653061225}\n\nid:\nwer=6.25 and normalized_wer=2.5\n{'eval/wer': 6.25, 'eval/normalized_wer': 2.5}\n\nit:\nwer=11.55440414507772 and normalized_wer=4.299443601416288\n{'eval/wer': 11.55440414507772, 'eval/normalized_wer': 4.299443601416288}\n\nja:\nwer=80.0 and normalized_wer=75.0\n{'eval/wer': 80.0, 'eval/normalized_wer': 75.0}\n\nko: we cannot test korean there isn't sufficient data we used in train\n\nnl:\nwer=6.281156530408774 and normalized_wer=4.633781763826607\n{'eval/wer': 6.281156530408774, 'eval/normalized_wer': 4.633781763826607}\n\npl:\nwer=12.931034482758621 and normalized_wer=2.2988505747126435\n{'eval/wer': 12.931034482758621, 'eval/normalized_wer': 2.2988505747126435}\n\npt:\nwer=39.55555555555556 and normalized_wer=14.847161572052403\n{'eval/wer': 39.55555555555556, 'eval/normalized_wer': 14.847161572052403}\n\nro:\nwer=11.440677966101696 and normalized_wer=9.243697478991598\n{'eval/wer': 11.440677966101696, 'eval/normalized_wer': 9.243697478991598}\n\nru:\nwer=5.911949685534592 and normalized_wer=3.258145363408521\n{'eval/wer': 5.911949685534592, 'eval/normalized_wer': 3.258145363408521}\n\ntr:\nwer=19.205298013245034 and normalized_wer=15.483870967741936\n{'eval/wer': 19.205298013245034, 'eval/normalized_wer': 15.483870967741936}\n\nuk:\nwer=16.593886462882097 and normalized_wer=7.048458149779736\n{'eval/wer': 16.593886462882097, 'eval/normalized_wer': 7.048458149779736}\n\nvi:\nwer=14.285714285714285 and normalized_wer=0.0\n{'eval/wer': 14.285714285714285, 'eval/normalized_wer': 0.0}\n\nzh-CN:\nwer=85.18518518518519 and normalized_wer=71.42857142857143\n{'eval/wer': 85.18518518518519, 'eval/normalized_wer': 71.42857142857143}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T19:40:09.327450Z","iopub.execute_input":"2025-06-29T19:40:09.327783Z","iopub.status.idle":"2025-06-29T19:40:09.705299Z","shell.execute_reply.started":"2025-06-29T19:40:09.327759Z","shell.execute_reply":"2025-06-29T19:40:09.704010Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_model_for_kbit_training\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WhisperForConditionalGeneration, Seq2SeqTrainer\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.15.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     19\u001b[0m     AutoPeftModel,\n\u001b[1;32m     20\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     21\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     22\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     23\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[1;32m     30\u001b[0m     PEFT_TYPE_TO_MIXED_MODEL_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     inject_adapter_in_model,\n\u001b[1;32m     34\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/auto.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     AutoModel,\n\u001b[1;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     PeftModel,\n\u001b[1;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m     35\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[1;32m     36\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[1;32m     37\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[1;32m     38\u001b[0m     PeftModelForSequenceClassification,\n\u001b[1;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mother\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_file_exists_on_hf_hub\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_file \u001b[38;5;28;01mas\u001b[39;00m safe_save_file\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'EncoderDecoderCache' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'EncoderDecoderCache' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T19:39:46.323098Z","iopub.execute_input":"2025-06-29T19:39:46.323769Z","iopub.status.idle":"2025-06-29T19:40:06.137050Z","shell.execute_reply.started":"2025-06-29T19:39:46.323741Z","shell.execute_reply":"2025-06-29T19:40:06.136102Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers\n  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.53.0-py3-none-any.whl (10.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed tokenizers-0.21.2 transformers-4.53.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"peft_model_id = \"fine-tuned-pt\" # Use the same model ID as before.\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(peft_model_id)\npeft_config = PeftConfig.from_pretrained(peft_model_id)\nmodel = WhisperForConditionalGeneration.from_pretrained(\n    peft_config.base_model_name_or_path,  device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(model, peft_model_id)\nmodel.config.use_cache = True\n\n\neval_dataloader = DataLoader(test_data, batch_size=5, collate_fn=data_collator)\nforced_decoder_ids = processor.get_decoder_prompt_ids(language=\"pt\",task=task)\nnormalizer = BasicTextNormalizer()\n\npredictions = []\nreferences = []\nnormalized_predictions = []\nnormalized_references = []\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.eval()\ni=0\nfor step, batch in enumerate(tqdm(eval_dataloader)):\n    with torch.cuda.amp.autocast():\n        with torch.no_grad():\n            #print(law_data_backup[\"sentence\"][i])\n            #lang_sentence = backup_law_data[\"sentence\"][i]\n            i = i + 1\n            input_features = batch[\"input_features\"].to(device)\n            #forced_decoder_ids = correct_lang_decoder(lang_sentence)\n            generated_tokens = (\n                    model.generate(\n                    input_features=input_features,\n                    forced_decoder_ids=forced_decoder_ids,\n                    max_new_tokens=255,\n                )\n                .cpu()\n                .numpy()\n            )\n            labels = batch[\"labels\"].to(device)\n            labels = labels.cpu().numpy()\n            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n            print(decoded_labels)\n            print(decoded_preds)\n            predictions.extend(decoded_preds)\n            references.extend(decoded_labels)\n            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n        del generated_tokens, labels, batch, input_features\n    gc.collect()\nwer = 100 * metric.compute(predictions=predictions, references=references)\nnormalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\neval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n\nprint(f\"{wer=} and {normalized_wer=}\")\nprint(eval_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T05:11:56.723905Z","iopub.execute_input":"2024-08-02T05:11:56.724303Z","iopub.status.idle":"2024-08-02T05:13:53.379892Z","shell.execute_reply.started":"2024-08-02T05:11:56.724270Z","shell.execute_reply":"2024-08-02T05:13:53.378978Z"},"trusted":true},"outputs":[{"name":"stderr","text":"  0%|          | 0/11 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"['exequenda', 'réus', 'lide, litígio, pleito judicial, conflituosa', 'à ordem do juízo será corrigida monetariamente', 'direito indisponível']\n[' Esse é o Coen.', ' Reus.', ' Lide, litígio, pleito judicial, conflituosa.', ' A ordem do juízo será corrigida monetariamente.', ' Direito disponível.']\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 2/11 [00:20<01:35, 10.59s/it]","output_type":"stream"},{"name":"stdout","text":"['cumprimento de sentença, provisório ou definitivo, na execução, resistida ou não', 'Legalização', 'intervenção federal, instrução, intimação, isonomia, togado, classista', 'judicial', 'dilação, prorrogação, diligência']\n[' Cumprimento de sentença provisória ou definitiva na execução, resistida ou não.', ' Legalização.', ' Intervenção Federal, Instrução, Intimação, Isonomia, Togado, Classista.', ' Jô de Ciao.', ' Delação, prorrogação, diligência.']\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 3/11 [00:31<01:26, 10.80s/it]","output_type":"stream"},{"name":"stdout","text":"['certificará', 'arbitral', 'semoventes, bis in idem', 'ou à excessiva dificuldade de cumprir o encargo nos termos do caput ou à', 'impugnante']\n[' Certificará.', ' Arbitral.', ' Simoventis, Bizin Idem', ' ou a excessiva dificuldade de cumprir o encargo nos termos do CAPT ou a', ' Impugnante.']\n['mediante ata lavrada por tabelião', 'divórcio', 'decretará', 'pretório, prevaricação, prevenção', 'registrada']\n[' Mediante ata lavrada por tabelião.', ' Divórcio.', ' Decreterá.', ' Pretório, Prevaricação, Prevenção.', ' Registrada.']\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 5/11 [00:47<00:53,  8.99s/it]","output_type":"stream"},{"name":"stdout","text":"['rescisória', 'denunciação caluniosa, lide, deportação', 'data venia, de facto, de jure', 'erga omnes', 'aresto, arguição, arresto']\n[' Recisória.', ' Denunciação caluniosa. Lide. Deportação.', ' Data venia, de facto, de júri.', ' Ergo hominis.', ' Aresto, argüição, arresto.']\n['pleiteada', 'revogação, desacato, desaforamento', 'impugnada', 'ilegítima', 'inépcia']\n[' Pleiteada.', ' Revogação, desacato, desaforamento.', ' Impugnada.', ' Ilegítima.', ' Inabissio.']\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▍    | 6/11 [00:54<00:42,  8.42s/it]","output_type":"stream"},{"name":"stdout","text":"['reverterá', 'a requerimento do réu, o juiz proferir sentença sem resolver o mérito', 'dolo, delito, omissão', 'na proporção de seu respectivo interesse na causa', 'litígios']\n[' Reverterá.', ' A requerimento do réu, o juiz proferir sentença sem resolver o mérito.', ' Dolo, delito, omissão.', ' na proporção de seu respectivo interesse na causa.', ' Litígios.']\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▎   | 7/11 [01:05<00:36,  9.24s/it]","output_type":"stream"},{"name":"stdout","text":"['elaboração de memória de cálculo, quando exigida para instauração da execução', 'impugnado', 'delituosa, ilícitos, auto-beneficiar, detrimento, induzir, manter', 'suspeito', 'alegando']\n[' Elaboração de memória de cálculo, quando exigida para instauração e da execução.', ' Impugnado.', ' Delituosa, ilícitos, autobeneficiar, detrimento, induzir, manter.', ' Suspeito.', ' Alegando.']\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 9/11 [01:26<00:19,  9.82s/it]","output_type":"stream"},{"name":"stdout","text":"['decisões interlocutórias', 'vinculante', 'descumprir', 'Salvo as disposições concernentes à gratuidade da justiça', 'o juiz aplicar-lhe-á a pena']\n[' Decisões interlocutórias.', ' Vinculante.', ' Descumprir.', ' Salvo as disposições concernentes à gratuidade da justiça.', ' O juiz aplicaria a pena.']\n['passível, ajustamento de conduta, antecipa, eficaz, descumprir, tipicidade, concretização, abstratamente', 'procuração', 'prejudicial', 'litisconsorcial', 'Os membros do Conselho podem designar expressamente um substituto em caso de ausência.']\n[' Passível, ajustamento de conduta, antecipa, eficaz, descumprir, tipicidade, concretização, abstratamente.', ' Procuração.', ' prejudicial.', ' Leeds Consorcial.', ' Os membros do Conselho podem designar expressamente um substituto em caso de ausência.']\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████ | 10/11 [01:41<00:11, 11.23s/it]","output_type":"stream"},{"name":"stdout","text":"['salvo se as invocar para eximir-se de obrigação legal a todos imposta', 'Vossa inabilidade ao cargo ficou evidente', 'Não cumprirei ordem ilegal', 'nenhuma pena passará da pessoa do condenado', 'não há crime sem lei anterior que o defina, nem pena sem cominação legal']\n[' salvo se as invocar para eximir-se de obrigação legal a todos imposta.', ' Vossa inabilidade ao cargo ficou evidente.', ' Não cumprirei ordem ilegal.', ' Nenhuma pena passará da pessoa do condenado', ' Não há crime sem lei anterior que o defina, nem pena sem combinação legal.']\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11/11 [01:51<00:00, 10.13s/it]","output_type":"stream"},{"name":"stdout","text":"wer=45.33333333333333 and normalized_wer=15.283842794759824\n{'eval/wer': 45.33333333333333, 'eval/normalized_wer': 15.283842794759824}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\"\"\"\nhe only:\nwer=20.51282051282051 and normalized_wer=7.6923076923076925\n{'eval/wer': 20.51282051282051, 'eval/normalized_wer': 7.6923076923076925}\n\nhi only:\nwer=20.689655172413794 and normalized_wer=2.0408163265306123\n{'eval/wer': 20.689655172413794, 'eval/normalized_wer': 2.0408163265306123}\n\nid only:\nwer=8.75 and normalized_wer=5.0\n{'eval/wer': 8.75, 'eval/normalized_wer': 5.0}\n\nit only:\nwer=10.414507772020725 and normalized_wer=4.704097116843703\n{'eval/wer': 10.414507772020725, 'eval/normalized_wer': 4.704097116843703}\n\nja only:\nwer=60.0 and normalized_wer=45.0\n{'eval/wer': 60.0, 'eval/normalized_wer': 45.0}\n\nnl only:\nwer=5.8325024925224325 and normalized_wer=4.733432984554061\n{'eval/wer': 5.8325024925224325, 'eval/normalized_wer': 4.733432984554061}\n\npl only:\nwer=12.35632183908046 and normalized_wer=1.4367816091954022\n{'eval/wer': 12.35632183908046, 'eval/normalized_wer': 1.4367816091954022}\n\npt only:\nwer=45.33333333333333 and normalized_wer=15.283842794759824\n{'eval/wer': 45.33333333333333, 'eval/normalized_wer': 15.283842794759824}\n\n\nro only:\nwer=9.745762711864407 and normalized_wer=8.403361344537815\n{'eval/wer': 9.745762711864407, 'eval/normalized_wer': 8.403361344537815}\n\nru only:\nwer=6.289308176100629 and normalized_wer=3.634085213032581\n{'eval/wer': 6.289308176100629, 'eval/normalized_wer': 3.634085213032581}\n\ntr only:\nwer=13.90728476821192 and normalized_wer=9.67741935483871\n{'eval/wer': 13.90728476821192, 'eval/normalized_wer': 9.67741935483871}\n\nuk only:\nwer=18.340611353711793 and normalized_wer=9.691629955947137\n{'eval/wer': 18.340611353711793, 'eval/normalized_wer': 9.691629955947137}\n\nvi only:\nwer=14.285714285714285 and normalized_wer=0.0\n{'eval/wer': 14.285714285714285, 'eval/normalized_wer': 0.0}\n\nzh-CN only:\nwer=88.88888888888889 and normalized_wer=62.857142857142854\n{'eval/wer': 88.88888888888889, 'eval/normalized_wer': 62.857142857142854}\n\"\"\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_domain_percentage(file_name):\n    domains = [\n        \"Domain: agriculture\",\n        \"Domain: education\",\n        \"Domain: history\",\n        \"Domain: language\",\n        \"Domain: music\"\n    ]\n    \n    domain_counts = {domain: 0 for domain in domains}\n    total_lines = 0\n    \n    with open(file_name, 'r') as file:\n        for line in file:\n            total_lines += 1\n            for domain in domains:\n                if domain in line:\n                    domain_counts[domain] += 1\n    \n    if total_lines == 0:\n        print(\"The file is empty.\")\n        return\n    \n    for domain, count in domain_counts.items():\n        percentage = (count / total_lines) * 100\n        print(f\"{domain}: {percentage:.3f}%\")\n\n# Example usage\nfile_name = \"/kaggle/input/dom-ar/domains_ar.txt\"\ncalculate_domain_percentage(file_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T13:00:53.580189Z","iopub.execute_input":"2024-08-05T13:00:53.580615Z","iopub.status.idle":"2024-08-05T13:00:53.650643Z","shell.execute_reply.started":"2024-08-05T13:00:53.580584Z","shell.execute_reply":"2024-08-05T13:00:53.649167Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Domain: agriculture: 0.004%\nDomain: education: 0.032%\nDomain: history: 0.106%\nDomain: language: 0.004%\nDomain: music: 0.004%\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport torch\nimport evaluate\nimport torchaudio\nfrom datasets import load_from_disk, Dataset, DatasetDict, Audio\nfrom transformers import (\n    WhisperFeatureExtractor,\n    WhisperTokenizer,\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    EarlyStoppingCallback\n)\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nfrom transformers.models.whisper.english_normalizer import BasicTextNormalizer\n\n# --- 1. Ortam Ayarları ---\nTARGET_LANG = \"he\"\nTARGET_LANG_CODE = \"hebrew\"\nREAD_ONLY_DATASET_PATH = \"/kaggle/input/he-dataset/he_dataset\"\nWRITABLE_DATASET_PATH = f\"/kaggle/working/datasets/{TARGET_LANG}_dataset\"\nMODEL_OUTPUT_DIR = f\"./whisper-large-v3-{TARGET_LANG}-finetuned\"\nCACHE_DIR = f\"/kaggle/working/cache_{TARGET_LANG}\"\n\nos.makedirs(CACHE_DIR, exist_ok=True)\nos.environ[\"HF_DATASETS_CACHE\"] = CACHE_DIR\nos.environ[\"TRANSFORMERS_CACHE\"] = CACHE_DIR\n\nmodel_name_or_path = \"openai/whisper-large-v3\"\ntask = \"transcribe\"\n\n# --- 2. Veri Setini Kopyala ve Yükle ---\nif not os.path.exists(WRITABLE_DATASET_PATH):\n    shutil.copytree(READ_ONLY_DATASET_PATH, WRITABLE_DATASET_PATH)\n\ndataset = load_from_disk(WRITABLE_DATASET_PATH)\nprint(dataset[\"train\"].column_names)\n\n# --- 3. Dataset Tipini Ayarla ve Ses Formatına Dönüştür ---\nif isinstance(dataset, Dataset):\n    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n    test_val_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n    dataset = DatasetDict({\n        \"train\": train_test_split[\"train\"],\n        \"validation\": test_val_split[\"train\"],\n        \"test\": test_val_split[\"test\"]\n    })\n\n# `path` sütununu ses formatına çevir\ndataset = dataset.cast_column(\"path\", Audio())\n\n# --- 4. Whisper İşleyicileri ---\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\ntokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=TARGET_LANG_CODE, task=task)\nprocessor = WhisperProcessor.from_pretrained(model_name_or_path, language=TARGET_LANG_CODE, task=task)\n\n# --- 5. Veri Hazırlama Fonksiyonu ---\ndef prepare_dataset(batch):\n    audio = batch[\"path\"]\n\n    if audio is None:\n        raise ValueError(f\"Ses verisi yüklenemedi: {batch}\")\n\n    batch[\"input_features\"] = feature_extractor(\n        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n\n\n# --- 6. Veriyi Map Et (Tokenizasyon ve Feature Extraction) ---\ntokenized_dataset = dataset.map(\n    prepare_dataset,\n    remove_columns=list(dataset[\"train\"].column_names),\n    num_proc=1,\n    batched=False,\n    load_from_cache_file=False\n)\n\n\n# --- 7. Data Collator ---\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n        batch[\"labels\"] = labels\n        return batch\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n\n# --- 8. Model Yükleme ve LoRA Ayarı ---\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=TARGET_LANG_CODE, task=task)\nmodel.config.suppress_tokens = []\n\nmodel = prepare_model_for_kbit_training(model)\nconfig = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n\n# --- 9. Eğitim Parametreleri ---\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=MODEL_OUTPUT_DIR,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    learning_rate=5e-6,\n    warmup_steps=50,\n    num_train_epochs=10,\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    fp16=True,\n    gradient_checkpointing=True,\n    report_to=\"tensorboard\",\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n)\n\n# --- 10. Değerlendirme Fonksiyonu ---\nmetric = evaluate.load(\"wer\")\nnormalizer = BasicTextNormalizer()\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n    pred_str = [normalizer(text) for text in pred_str]\n    label_str = [normalizer(text) for text in label_str]\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n    return {\"wer\": wer}\n\n# --- 11. Trainer ve Eğitim ---\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\nprint(\"🚀 Eğitim başlıyor...\")\ntrainer.train()\nprint(\"✅ Eğitim tamamlandı.\")\n\n# --- 12. Modeli Kaydet ---\ntrainer.save_model(MODEL_OUTPUT_DIR)\nprint(f\"💾 En iyi model {MODEL_OUTPUT_DIR} dizinine kaydedildi.\")\n\n# --- 13. Nihai Test Değerlendirmesi ---\nprint(\"\\n📊 Test seti üzerinde değerlendirme:\")\ntest_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\nprint(test_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T14:47:46.046714Z","iopub.execute_input":"2025-06-28T14:47:46.047061Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"375ca64ac2594a75b3bbecae6e58a4e3"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
